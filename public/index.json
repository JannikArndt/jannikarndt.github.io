[{"content":"Scala traits should define defs, because\n “A val can override a def, but a def cannot override a val”\n (via Alvin Alexander via StackOverflow).\nBut it\u0026rsquo;s interesting to look at how the compiler treats that. Alvin Alexander did that, but only for Scala 2.12.8. Let\u0026rsquo;s have a look at 2.12, 2.13 and the just released 3.0 (formerly known as dotty).\nGetting the scalac versions You can install different versions of the Scala compiler via brew:\n$ brew install scala@2.12 $ brew install scala # currently for 2.13 $ brew install dotty # currently Scala 3.0.0-RC2  Note: you can only have one scala and scalac on your path, so you want to keep the versions you don\u0026rsquo;t use unlinked (or \u0026ldquo;keg-only\u0026rdquo; in brew-speak).\n You can access the versions via\n$ /usr/local/Cellar/scala@2.12/2.12.13/bin/scalac -version # one dash only! Scala compiler version 2.12.13 -- Copyright 2002-2020, LAMP/EPFL and Lightbend, Inc. $ /usr/local/Cellar/scala/2.13.5/bin/scalac --version Scala compiler version 2.13.5 -- Copyright 2002-2020, LAMP/EPFL and Lightbend, Inc. $ /usr/local/Cellar/dotty/3.0.0-RC3/bin/scalac --version Scala compiler version 3.0.0-RC3 -- Copyright 2002-2021, LAMP/EPFL At the end of scalac As you might know, the compiler works in multiple steps (\u0026ldquo;phases\u0026rdquo;)., and it can output the result after each step. The -Xshow-phases option prints out an overview of the compile-phases:\n  Here they are written out, if you want to read through them:  Scala 2.12.13 $ /usr/local/Cellar/scala@2.12/2.12.13/bin/scalac -Xshow-phases phase name id description ---------- -- ----------- parser 1 parse source into ASTs, perform simple desugaring namer 2 resolve names, attach symbols to named trees packageobjects 3 load package objects typer 4 the meat and potatoes: type the trees patmat 5 translate match expressions superaccessors 6 add super accessors in traits and nested classes extmethods 7 add extension methods for inline classes pickler 8 serialize symbol tables refchecks 9 reference/override checking, translate nested objects uncurry 10 uncurry, translate function values to anonymous classes fields 11 synthesize accessors and fields, add bitmaps for lazy vals tailcalls 12 replace tail calls by jumps specialize 13 @specialized-driven class and method specialization explicitouter 14 this refs to outer pointers erasure 15 erase types, add interfaces for traits posterasure 16 clean up erased inline classes lambdalift 17 move nested functions to top level constructors 18 move field definitions into constructors flatten 19 eliminate inner classes mixin 20 mixin composition cleanup 21 platform-specific cleanups, generate reflective calls delambdafy 22 remove lambdas jvm 23 generate JVM bytecode terminal 24 the last phase during a compilation run Scala 2.13.5 /usr/local/Cellar/scala/2.13.5/bin/scalac -Xshow-phases phase name id description ---------- -- ----------- parser 1 parse source into ASTs, perform simple desugaring namer 2 resolve names, attach symbols to named trees packageobjects 3 load package objects typer 4 the meat and potatoes: type the trees superaccessors 5 add super accessors in traits and nested classes extmethods 6 add extension methods for inline classes pickler 7 serialize symbol tables refchecks 8 reference/override checking, translate nested objects patmat 9 translate match expressions uncurry 10 uncurry, translate function values to anonymous classes fields 11 synthesize accessors and fields, add bitmaps for lazy vals tailcalls 12 replace tail calls by jumps specialize 13 @specialized-driven class and method specialization explicitouter 14 this refs to outer pointers erasure 15 erase types, add interfaces for traits posterasure 16 clean up erased inline classes lambdalift 17 move nested functions to top level constructors 18 move field definitions into constructors flatten 19 eliminate inner classes mixin 20 mixin composition cleanup 21 platform-specific cleanups, generate reflective calls delambdafy 22 remove lambdas jvm 23 generate JVM bytecode terminal 24 the last phase during a compilation run Scala 3.0.0-RC3 aka dotty /usr/local/Cellar/dotty/3.0.0-RC3/bin/scalac -Xshow-phases typer inlinedPositions sbt-deps extractSemanticDB posttyper prepjsinterop sbt-api SetRootTree pickler inlining postInlining staging pickleQuotes {firstTransform, checkReentrant, elimPackagePrefixes, cookComments, checkStatic, betaReduce, inlineVals, expandSAMs, initChecker} {elimRepeated, protectedAccessors, extmethods, uncacheGivenAliases, byNameClosures, hoistSuperArgs, specializeApplyMethods, refchecks} {elimOpaque, tryCatchPatterns, patternMatcher, explicitJSClasses, explicitOuter, explicitSelf, elimByName, stringInterpolatorOpt} {pruneErasedDefs, inlinePatterns, vcInlineMethods, seqLiterals, intercepted, getters, specializeFunctions, liftTry, collectNullableFields, elimOuterSelect, resolveSuper, functionXXLForwarders, paramForwarding, genericTuples, letOverApply, arrayConstructors} erasure {elimErasedValueType, pureStats, vcElideAllocations, arrayApply, addLocalJSFakeNews, elimPolyFunction, tailrec, completeJavaEnums, mixin, lazyVals, memoize, nonLocalReturns, capturedVars} {constructors, instrumentation} {lambdaLift, elimStaticThis, countOuterAccesses} {dropOuterAccessors, flatten, renameLifted, transformWildcards, moveStatic, expandPrivate, restoreScopes, selectStatic, junitBootstrappers, collectSuperCalls, repeatableAnnotations} genSJSIR genBCode  Compile results and the result is\n$ /usr/local/Cellar/dotty/3.0.0-RC3/bin/scalac -Xprint:all Main.scala […] result of Main.scala after MegaPhase{dropOuterAccessors, flatten, renameLifted, transformWildcards, moveStatic, expandPrivate, restoreScopes, selectStatic, collectSuperCalls, repeatableAnnotations}: package \u0026lt;empty\u0026gt; { @scala.annotation.internal.SourceFile(\u0026#34;Main.scala\u0026#34;) class MyClass extends Object , MyTrait { def \u0026lt;init\u0026gt;(): Unit = { super() this.id = 1 () } private val id: Int def id(): Int = this.id } @scala.annotation.internal.SourceFile(\u0026#34;Main.scala\u0026#34;) trait MyTrait() extends Object { def id(): Int } } Example Code I combined the function, abstract value and concrete value into one example:\ntrait MyTrait { def id1: Int // function  val id2: Int // abstract value  val id3: Int = 3 // concrete value } class MyClass extends MyTrait { val id1 = 1 val id2 = 2 override val id3 = 3 } Results Here are the results:\n Let\u0026rsquo;s unpack this:\n1. Scala 2.12 and 2.13 have the same output: package \u0026lt;empty\u0026gt; { abstract trait MyTrait extends Object { \u0026lt;accessor\u0026gt; \u0026lt;sub_synth\u0026gt; protected[this] def MyTrait$_setter_$id3_=(x$1: Int): Unit; def id1(): Int; \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id2(): Int; \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; \u0026lt;sub_synth\u0026gt; def id3(): Int; def /*MyTrait*/$init$(): Unit = { MyTrait.this.MyTrait$_setter_$id3_=((3: Int)); () } }; class MyClass extends Object with MyTrait { override \u0026lt;accessor\u0026gt; protected[this] def MyTrait$_setter_$id3_=(x$1: Int): Unit = (); private[this] val id1: Int = _; \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id1(): Int = MyClass.this.id1; private[this] val id2: Int = _; \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id2(): Int = MyClass.this.id2; private[this] val id3: Int = _; override \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id3(): Int = MyClass.this.id3; def \u0026lt;init\u0026gt;(): MyClass = { MyClass.super.\u0026lt;init\u0026gt;(); MyClass.super./*MyTrait*/$init$(); MyClass.this.id1 = 1; MyClass.this.id2 = 2; MyClass.this.id3 = 3; () } } } In the trait, the function\ndef id1: Int becomes\ndef id1(): Int; the abstract val\nval id2: Int becomes\n\u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id2(): Int; and the concrete val\nval id3: Int = 3 needs the most work:\n\u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; \u0026lt;sub_synth\u0026gt; def id3(): Int; \u0026lt;accessor\u0026gt; \u0026lt;sub_synth\u0026gt; protected[this] def MyTrait$_setter_$id3_=(x$1: Int): Unit; def /*MyTrait*/$init$(): Unit = { MyTrait.this.MyTrait$_setter_$id3_=((3: Int)); () } (The constructor would not be present if the trait only contained id1 and id2.)\nWhat do \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; \u0026lt;sub_synth\u0026gt; mean exactly? I have no idea. Neither does Google.\nIn the class we have private vals for all three:\nprivate[this] val id1: Int = _; \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id1(): Int = MyClass.this.id1; private[this] val id2: Int = _; \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id2(): Int = MyClass.this.id2; private[this] val id3: Int = _; override \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; def id3(): Int = MyClass.this.id3; and the MyTrait$_setter_$id3_ from the trait is overridden:\noverride \u0026lt;accessor\u0026gt; protected[this] def MyTrait$_setter_$id3_=(x$1: Int): Unit = (); So in summary: So far, there\u0026rsquo;s not much of a difference.\n2. Scala 3 has syntax-highlighting! After the compile-phases-output was so ugly, it\u0026rsquo;s nice to see that the output of the phases is actually colored.\n3. There\u0026rsquo;s an annotation @scala.annotation.internal.SourceFile(\u0026#34;Main.scala\u0026#34;) points to the source file. Neat.\n4. \u0026lt;stable\u0026gt; \u0026lt;accessor\u0026gt; \u0026lt;sub_synth\u0026gt; are gone I guess now I\u0026rsquo;ll never find out what they were…\nThis also means that all fields in the trait are now exactly the same:\ndef id1(): Int def id2(): Int def id3(): Int and the setter-function for id3 goes from\n\u0026lt;accessor\u0026gt; \u0026lt;sub_synth\u0026gt; protected[this] def MyTrait$_setter_$id3_=(x$1: Int): Unit; to\ndef MyTrait$_setter_$id3_$eq(x$0: Int): Unit In the class, the three are also very much alike:\nprivate val id1: Int def id1(): Int = this.id1 private val id2: Int def id2(): Int = this.id2 private var id3: Int override def id3(): Int = this.id3 Bytecode deep-dive Let\u0026rsquo;s take one step further and take the actual Bytecode apart, using javap -c:\n Observations:\n Scala 2.12 and 2.13 result in the exact same Bytecode Scala 3 places the constructor first The MyTrait$_setter_$id3_$eq(int) actually contains some setting-code now! public void MyTrait$_setter_$id3_$eq(int); Code: 0: aload_0 1: iload_1 2: putfield #25 // Field id3:I 5: return as compared to\npublic void MyTrait$_setter_$id3_$eq(int); Code: 0: return  the scala.runtime.Statics$#releaseFence() has a single jvm-operation (invokestatic) resulting from it.  Next Level: JVM 17 The compiler can also target a specific version of the Java platform, using the -target:8 (-Xtarget for Scala 3) or -release option. However, for this simple example, there were no differences in the bytecode.\nSummary So, what to make of this? Easy: Using def or val in traits is purely a developer ergonomics decision, not a runtime difference.\n","permalink":"https://www.jannikarndt.de/blog/2021/04/def_and_var_in_trait/","summary":"\u003cp\u003eScala \u003ccode\u003etrait\u003c/code\u003es should define \u003ccode\u003edef\u003c/code\u003es, because\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“A val can override a def, but a def cannot override a val”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e(via \u003ca href=\"https://alvinalexander.com/scala/fp-book/def-vs-val-abstract-members-traits-classes-override/\"\u003eAlvin Alexander\u003c/a\u003e via \u003ca href=\"https://stackoverflow.com/questions/13126104/is-there-any-advantage-to-definining-a-val-over-a-def-in-a-trait\"\u003eStackOverflow\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eBut it\u0026rsquo;s interesting to look at how the \u003cem\u003ecompiler\u003c/em\u003e treats that.\nAlvin Alexander \u003ca href=\"https://alvinalexander.com/scala/all-wanted-to-know-about-def-val-var-fields-in-traits/\"\u003edid that\u003c/a\u003e, but only for Scala 2.12.8.\nLet\u0026rsquo;s have a look at 2.12, 2.13 and the just released 3.0 (formerly known as dotty).\u003c/p\u003e","title":"def and var in traits on Scala 2.12, 2.13 and 3.0.0"},{"content":"We recently had another team rewrite their service in Rust, with a massive boost in performance. So naturally, for April Fools' I made a big announcement that our team followed suit:\nBut, at good prank isn\u0026rsquo;t just \u0026ldquo;I thought this was true, but it wasn\u0026rsquo;t\u0026rdquo;. It\u0026rsquo;s \u0026ldquo;I though it was a joke, but for a brief moment doubted that\u0026rdquo;. The \u0026ldquo;Oh, no, he didn\u0026rsquo;t\u0026quot;-moment.\nAlas, one more line was necessary:\nThe proof. With a link. A Call-To-Action. Click it! Marvel at it!\nCreating fake commits Creating a GitHub repo that looks like Rust isn\u0026rsquo;t hard. You can look at a few open-source projects and copy what you need (respecting the license, of course).\nThat gives you the language-overview:\n  And a proper Cargo.toml, prominently featuring dependencies that seem to make sense:\nI committed using the --date flag:\n$ git commit -a -m \u0026#34;Initial template \u0026amp; infra for booking3\u0026#34; \\ --date=\u0026#34;7 days ago at 08:46\u0026#34; and checked the log:\n$ git log commit 983e4977983ee134771885f6c08881513c4faef1 (HEAD -\u0026gt; main) Author: Jannik Arndt \u0026lt;jannik@jannikarndt.de\u0026gt; Date: Mon Mar 22 08:46:00 2021 +0100 Initial template \u0026amp; infra for booking3 Looks good. For a preview, I created a repo in my personal account and pushed it:\n$ git remote add origin https://github.com/JannikArndt/booking3.git $ git push --set-upstream origin main   TWO MINUTES AGO? How could that be?!\ngit has different formats for the log: oneline, short, medium, full, fuller, reference, email and raw. The default is medium.\nOnly the fuller format shows the whole truth:\n$ git log --pretty=fuller commit 983e4977983ee134771885f6c08881513c4faef1 (HEAD -\u0026gt; main, origin/main) Author: Jannik Arndt \u0026lt;jannik@jannikarndt.de\u0026gt; AuthorDate: Mon Mar 22 08:46:00 2021 +0100 Commit: Jannik Arndt \u0026lt;jannik@jannikarndt.de\u0026gt; CommitDate: Tue Mar 30 22:47:39 2021 +0200 Initial template \u0026amp; infra for booking3 There\u0026rsquo;s an AuthorDate and a CommitDate!\nOkay, let\u0026rsquo;s try again.\nFaking AuthorDate and CommitDate A quick googling later I arrived at\n$ GIT_COMMITTER_DATE=\u0026#34;2021-03-20T11:43:11\u0026#34; \\ git commit -a -m \u0026#34;Initial template \u0026amp; infra for booking3\u0026#34; \\ --date=\u0026#34;2021-03-20T11:43:11\u0026#34; Force-push aaaannd\n  Nice 👍\nFaking the Author — and Committer Next I needed commits from the whole team to make this look realistic. Following the ENV-vars:\n$ GIT_COMMITTER_DATE=\u0026#34;2021-03-23T11:43:11\u0026#34; \\ GIT_COMMITTER_NAME=\u0026#34;Cristina\u0026#34; \\ GIT_COMMITTER_EMAIL=\u0026#34;cristina@moia.io\u0026#34; \\ git commit -a -m \u0026#34;Change deployment to booking3\u0026#34; --date=\u0026#34;2021-03-23T11:43:11\u0026#34; Of course not.\n  GitHub does make a difference between the author and the committer. So to fully fake someone else\u0026rsquo;s commit:\n$ GIT_COMMITTER_DATE=\u0026#34;2021-03-23T11:43:11\u0026#34; \\ GIT_COMMITTER_NAME=\u0026#34;Cristina\u0026#34; \\ GIT_COMMITTER_EMAIL=\u0026#34;cristina@moia.io\u0026#34; \\ git commit -a -m \u0026#34;Change deployment to booking3\u0026#34; --date=\u0026#34;2021-03-23T11:43:11\u0026#34; \\ --author=\u0026#34;Cristina \u0026lt;cristina@moia.io\u0026gt;\u0026#34; It worked 😎\nSince I had a few commits left to do and I don\u0026rsquo;t enjoy typing, I refactored my command a little:\n$ DATE=\u0026#34;2021-03-23T11:43:11\u0026#34;; \\ NAME=\u0026#34;Cristina\u0026#34;; \\ MAIL=cristina@moia.io; \\ GIT_COMMITTER_DATE=$DATE GIT_COMMITTER_NAME=$NAME GIT_COMMITTER_EMAIL=$MAIL \\ git commit --author=\u0026#34;$NAME\u0026lt;$MAIL\u0026gt;\u0026#34; --date=$DATE -m \u0026#34;\u0026lt;the message\u0026gt;\u0026#34; Fine-Tuning I also tried --allow-empty, and while the messages appear in the history, they don\u0026rsquo;t change the appearance on the first view, since they don\u0026rsquo;t touch any files. Obviously.\nI also tried to balance the amount of lines to generate a more realistic \u0026ldquo;Pulse\u0026rdquo; in the \u0026ldquo;Insights\u0026rdquo; section:\n  (I still remembered from when I found the other teams Rust-repo that I looked at this to find out who was leading this effort.)\nNext I needed the green little checkmark to fabricate a successful CI/CD:\n  It hurts a little to spin up a fresh Ubuntu just for a green checkmark, but I didn\u0026rsquo;t find a quicker way:\nname:deploymenton:push:branches:[main]jobs:test:runs-on:ubuntu-20.04steps:- run:echo \u0026#34;April Fools!\u0026#34;I also thought about creating and merging PRs, but those are way harder to forcibly overwrite (and I don\u0026rsquo;t know the GitHub API well enough to automate that on the spot).\nResult After about two hours of hacking, I was satisfied.\nThe reactions were rewarding: People dug into the code, found, commented on and appreciated all the details and even opened new Pull-Requests.\nHowever, one thing nobody noticed…\nLearnings After figuring out how, creating fake commits for my co-workers turned out surprisingly easy. Or for anyone else, for that matter:\n  That\u0026rsquo;s not great. What you can do about it is verified commits. Since we use the \u0026ldquo;Squash and merge\u0026rdquo; button, all commits on master are done through the GitHub UI and therefore verified by our logins:\n  To also have this for commits you create locally, you can sign commits via GPG\n$ git config --global commit.gpgsign true and tell GitHub what your public key is.\n  ","permalink":"https://www.jannikarndt.de/blog/2021/04/how_to_fake_a_github_repo/","summary":"We recently had another team rewrite their service in Rust, with a massive boost in performance. So naturally, for April Fools' I made a big announcement that our team followed suit:\nBut, at good prank isn\u0026rsquo;t just \u0026ldquo;I thought this was true, but it wasn\u0026rsquo;t\u0026rdquo;. It\u0026rsquo;s \u0026ldquo;I though it was a joke, but for a brief moment doubted that\u0026rdquo;. The \u0026ldquo;Oh, no, he didn\u0026rsquo;t\u0026quot;-moment.\nAlas, one more line was necessary:","title":"April Fools': “We rewrote it in Rust” or How To Fake A GitHub Repo"},{"content":"Do you know what happens after you compile? Let\u0026rsquo;s take a look at what the Scala compiler tells us, and what scalap and javap can reveal about .class files.\nCreating a Minimal Example with sbt First, we\u0026rsquo;ll need something very small to compile, to still understand all of the output.\nbuild.sbt name := \u0026#34;scala-start\u0026#34; version := \u0026#34;1\u0026#34; scalaVersion := \u0026#34;2.13.5\u0026#34; scalacOptions := Seq(\u0026#34;-target:11\u0026#34;) project/build.properties sbt.version=1.4.9 src/main/scala/Main.scala object Main { def main(args: Array[String]) = { println(\u0026#34;Hello.\u0026#34;) } } run\n$ sbt package The result is in target/scala-2.13/scala-start_2.13-1.jar. You can look at/unpack the contents via\n$ jar -xvf target/scala-2.13/scala-start_2.13-1.jar inflated: META-INF/MANIFEST.MF inflated: Main$.class inflated: Main.class To run it via java, you need to add both the jar and the scala-library.jar to the classpath. If you use sbt stage, it gets copied next to the output, but you can also reference the one that is already present on your machine (in this example installed via Homebrew):\n$ java -cp \u0026#34;/usr/local/Cellar/scala/2.13.5/libexec/lib/*:.\u0026#34; Main Hello. Creating a Really Minimal Example (without sbt) To go even more minimal, you can work with a single scala-file:\nMain.scala object Main { def main(args: Array[String]) = { println(\u0026#34;Hello.\u0026#34;) } } Compile and run it via\nscala Main.scala Hello. or\n$ scalac Main.scala $ scala Main Hello. The second version will create two class files (that you also found inside the jar earlier):\n Main.class Main$.class (the object)  Running scalap and javap Now that you have class-files, you can take them apart with\n$ scalap Main Main$ object Main extends scala.AnyRef { def this() = { /* compiled code */ } def main(args: scala.Array[scala.Predef.String]): scala.Unit = { /* compiled code */ } } package Main$; final class Main$ extends scala.AnyRef { def this(): scala.Unit; def main(scala.Array[java.lang.String]): scala.Unit; } object Main$ { final val MODULE$: Main$; } You can also use javap, since it\u0026rsquo;s just class-files:\n$ javap Main Main$ Compiled from \u0026#34;Main.scala\u0026#34; public final class Main { public static void main(java.lang.String[]); } Compiled from \u0026#34;Main.scala\u0026#34; public final class Main$ { public static final Main$ MODULE$; public static {}; public void main(java.lang.String[]); } Excursion: Scala 3 / dotty If you compile with the Scala 3 compiler (dotty), the result of scalap will be slightly different:\n$ /usr/local/Cellar/dotty/3.0.0-RC1/bin/scalac Main.scala $ scalap Main Main$ package Main; final class Main extends scala.AnyRef { } object Main { def main(scala.Array[java.lang.String]): scala.Unit; } package Main$; final class Main$ extends scala.AnyRef with java.io.Serializable { def main(scala.Array[java.lang.String]): scala.Unit; def writeReplace(): scala.Any; def this(): scala.Unit; } object Main$ { final val MODULE$: Main$; } …while the javap output stays almost the same (except for implements java.io.Serializable).\n$ javap Main Main$ Compiled from \u0026#34;Main.scala\u0026#34; public final class Main { public static void main(java.lang.String[]); } Compiled from \u0026#34;Main.scala\u0026#34; public final class Main$ implements java.io.Serializable { public static final Main$ MODULE$; public static {}; public void main(java.lang.String[]); } Printing just before JVM Bytecode Generation The Scala compiler allows you to get a preview with the print flag:\n$ scalac -print:true Main.scala [[syntax trees at end of cleanup]] // Main.scala package \u0026lt;empty\u0026gt; { object Main extends Object { def main(args: Array[String]): Unit = scala.Predef.println(\u0026#34;Hello.\u0026#34;); def \u0026lt;init\u0026gt;(): Main.type = { Main.super.\u0026lt;init\u0026gt;(); () } } } The \u0026ldquo;cleanup\u0026rdquo; refers to the phase of the compiler. You can see them with\n$ scalac -Vphases // or -Xshow-phases phase name id description ---------- -- ----------- parser 1 parse source into ASTs, perform simple desugaring namer 2 resolve names, attach symbols to named trees packageobjects 3 load package objects typer 4 the meat and potatoes: type the trees superaccessors 5 add super accessors in traits and nested classes extmethods 6 add extension methods for inline classes pickler 7 serialize symbol tables refchecks 8 reference/override checking, translate nested objects patmat 9 translate match expressions uncurry 10 uncurry, translate function values to anonymous classes fields 11 synthesize accessors and fields, add bitmaps for lazy vals tailcalls 12 replace tail calls by jumps specialize 13 @specialized-driven class and method specialization explicitouter 14 this refs to outer pointers erasure 15 erase types, add interfaces for traits posterasure 16 clean up erased inline classes lambdalift 17 move nested functions to top level constructors 18 move field definitions into constructors flatten 19 eliminate inner classes mixin 20 mixin composition cleanup 21 platform-specific cleanups, generate reflective calls delambdafy 22 remove lambdas jvm 23 generate JVM bytecode terminal 24 the last phase during a compilation run You can also get the output after a specific compiler phase with\n$ scalac -Vprint:24 Main.scala [[syntax trees at end of pickler]] // Main.scala package \u0026lt;empty\u0026gt; { object Main extends scala.AnyRef { def \u0026lt;init\u0026gt;(): Main.type = { Main.super.\u0026lt;init\u0026gt;(); () }; def main(args: Array[String]): Unit = scala.Predef.println(\u0026#34;Hello.\u0026#34;) } } You can also specify ranges:\n$ scalac -Vprint:1-5 Main.scala or all the steps with underscore:\n$ scalac -Vprint:_ Main.scala You can find more information about the compiler-flags\n by running scalac, scalac -V, scalac -X, etc in the docs on this website at the source of truth aka the code  The Bytecode The final step is to see the actual bytecode:\n$ javap -private -c -s -l Main Compiled from \u0026#34;Main.scala\u0026#34; public final class Main { public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V Code: 0: getstatic #17 // Field Main$.MODULE$:LMain$;  3: aload_0 4: invokevirtual #19 // Method Main$.main:([Ljava/lang/String;)V  7: return } You can also use a tool such as jclasslib to inspect the bytecode.\nThe Java Code Okay, Bytecode was not the final step, you can take one further and convert it back to Java code! I use cfr for that, and add the scala-library to the classpath:\n$ java -jar cfr.jar Main Main$ --extraclasspath /usr/local/Cellar/scala/2.13.5/libexec/lib/scala-library.jar /* * Decompiled with CFR 0.151. */ import scala.reflect.ScalaSignature; @ScalaSignature(bytes=\u0026#34;\\u0006\\u0005... (this is \u0026#39;pickled scala\u0026#39;)\u0026#34;) public final class Main { public static void main(String[] stringArray) { Main$.MODULE$.main(stringArray); } } /* * Decompiled with CFR 0.151. */ import scala.Predef$; public final class Main$ { public static final Main$ MODULE$ = new Main$(); public void main(String[] args) { Predef$.MODULE$.println(\u0026#34;Hello.\u0026#34;); } private Main$() { } } Errors you can run into NoClassDefFoundError: scala/Predef$ Exception in thread \u0026#34;main\u0026#34; java.lang.NoClassDefFoundError: scala/Predef$ at Main$.main(Main.scala:3) at Main.main(Main.scala) Caused by: java.lang.ClassNotFoundException: scala.Predef$ at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:606) at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:168) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522) =\u0026gt; your jar is missing the scala-library. Either package it (using sbt stage for example) or add it to the classpath. Either way you probably want to use\n$ java -cp \u0026quot;path/to/scala/lib/*:path/to/your/jar\u0026quot; NameOfTheMainClass for example\n$ java -cp \u0026quot;/usr/local/Cellar/scala/2.13.5/libexec/lib/*:target/scala-2.13/scala-start_2.13-1.jar\u0026quot; Main NoClassDefFoundError: scala/Function0 Error: Unable to initialize main class Main Caused by: java.lang.NoClassDefFoundError: scala/Function0 This is the same as above, but you used the\nobject Main extends App { println(\u0026#34;Hello.\u0026#34;) } code, which gives a slightly nicer error message. The solution is the same: put the scala-library and your jar in the classpath.\nNo such file or class on classpath: Main $ scala Main No such file or class on classpath: Main The scala command can do two things:\n run something from a *.class file if you provide a name compile and run something if you provide a file name  Solution: Either run $ scala Main.scala or $ scalac Main.scala first and then $ scala Main.\nclass/object Main not found. $ scalap Main class/object Main not found. $ scalap Main.scala class/object Main.scala not found. The scalap tool actually needs compiled classes. Run $ scalac Main.scala first.\n","permalink":"https://www.jannikarndt.de/blog/2021/03/using_scalap_javap_and_the_scala_compiler/","summary":"\u003cp\u003eDo you know what happens after you compile? Let\u0026rsquo;s take a look at what the Scala compiler tells us, and what \u003ccode\u003escalap\u003c/code\u003e and \u003ccode\u003ejavap\u003c/code\u003e can reveal about \u003ccode\u003e.class\u003c/code\u003e files.\u003c/p\u003e","title":"Using scalap, javap and the scala compiler to understand the journey from code to bytecode"},{"content":"I recently created a wonderful bug.\nHave a look at this code and tell me: Do you need to look out for exceptions?\nimport scala.concurrent.ExecutionContext.Implicits.global import scala.concurrent.Future import scala.util.{Failure, Success} object Main extends App { val myFuture: Future[Int] = for { foo \u0026lt;- doFoo() bar \u0026lt;- doBar() } yield foo + bar def doFoo(): Future[Int] = ??? def doBar(): Future[Int] = ??? myFuture onComplete { case Success(value) =\u0026gt; println(s\u0026#34;It\u0026#39;s a $value\u0026#34;) case Failure(exception) =\u0026gt; println(s\u0026#34;Something went wrong: $exception\u0026#34;) } } Nahhh, probably not, because\nFuture{ throw new Throwable } is the same as\nFuture.apply(throw new Throwable) is the same as\nFuture.unit.map(_ =\u0026gt; throw new Throwable) is the same as\nFuture.successful(()).transform(_ map (_ =\u0026gt; throw new Throwable)) and this transform is defined as\ndef transform[S](f: Try[T] =\u0026gt; Try[S])(implicit executor: ExecutionContext): Future[S] and therefore the map is defined as\ndef map[U](f: T =\u0026gt; U): Try[U] and Try has the constructor\nobject Try { /** Constructs a `Try` using the by-name parameter. This * method will ensure any non-fatal exception is caught and a * `Failure` object is returned. */ def apply[T](r: =\u0026gt; T): Try[T] = try Success(r) catch { case NonFatal(e) =\u0026gt; Failure(e) } } and there we finally have our try-catch-block! So we don\u0026rsquo;t have to care about exceptions in Futures, right?\nThe Catch While the conclusion is correct, and exceptions inside futures will be caught, not all the code is inside a future!\nThe for comprehension de-sugared is the same as\ndoFoo().flatMap(foo =\u0026gt; doBar().map(bar =\u0026gt; foo + bar)) and although doFoo() returns a Future, there can be parts that are not inside that Future:\ndef doFoo(): Future[Int] = { throw new Throwable Future(3) } BÄM! Now there\u0026rsquo;s no try around the throw and the Throwable will bubble to the top and kill your program.\nThe Solution Obviously, when returning a Future you shouldn\u0026rsquo;t throw exceptions. But if you\u0026rsquo;re the caller and don\u0026rsquo;t have any influence on this, this will save you:\nval myFuture: Future[Int] = for { _ \u0026lt;- Future.unit foo \u0026lt;- doFoo() bar \u0026lt;- doBar() } yield foo + bar It\u0026rsquo;s pretty much the same way the standard library does it: Start with something successful and map your way from there!\nDe-sugared, this would be\nFuture.unit.flatMap(_ =\u0026gt; doFoo().flatMap(foo =\u0026gt; doBar().map(bar =\u0026gt; foo + bar))) What a mess, just to be on the safe side.\n","permalink":"https://www.jannikarndt.de/blog/2018/12/exception_proof_for_comprehensions/","summary":"\u003cp\u003eI recently created a \u003cem\u003ewonderful\u003c/em\u003e bug.\u003c/p\u003e","title":"Exception-Proof For-Comprehensions"},{"content":"This is a basic example how to implement oAuth2 using Akka HTTP and Scala. It provides three endpoints. From the clients point of view:\n / — publicly accessible, returns “Welcome!”, /auth — provide your username and password, receive an access_token in return, /api — secured by oAuth, send the access_token in a header to gain access.  From the server\u0026rsquo;s point of view:\n / — publicly accessible, do nothing, /auth — receive basic auth credentials, verify they\u0026rsquo;re in the list of known credentials, create an access_token, return it, /api — receive authorization header, check if access_token is in list of valid tokens.  Since oAuth tokens are short lived, the server also has to invalidate expired tokens.\nbuild.sbt This minimal example requires the following imports in your build.sbt:\nname := \u0026#34;oAuth-example\u0026#34; scalaVersion := \u0026#34;2.12.7\u0026#34; version := \u0026#34;1.0\u0026#34; libraryDependencies ++= Seq( \u0026#34;com.typesafe.akka\u0026#34; %% \u0026#34;akka-http\u0026#34; % \u0026#34;10.1.5\u0026#34;, \u0026#34;com.typesafe.akka\u0026#34; %% \u0026#34;akka-stream\u0026#34; % \u0026#34;2.5.17\u0026#34;, \u0026#34;de.heikoseeberger\u0026#34; %% \u0026#34;akka-http-json4s\u0026#34; % \u0026#34;1.22.0\u0026#34;, \u0026#34;org.json4s\u0026#34; %% \u0026#34;json4s-native\u0026#34; % \u0026#34;3.6.1\u0026#34;, \u0026#34;com.typesafe.scala-logging\u0026#34; %% \u0026#34;scala-logging\u0026#34; % \u0026#34;3.9.0\u0026#34;, \u0026#34;org.slf4j\u0026#34; % \u0026#34;slf4j-simple\u0026#34; % \u0026#34;1.7.25\u0026#34; ) Start a WebServer The basic WebServer can be started using the following code:\nimport akka.http.scaladsl.server._ import com.typesafe.scalalogging.StrictLogging object Main { def main(args: Array[String]): Unit = { val port: Int = sys.env.getOrElse(\u0026#34;PORT\u0026#34;, \u0026#34;8080\u0026#34;).toInt WebServer.startServer(\u0026#34;0.0.0.0\u0026#34;, port) } } object WebServer extends HttpApp with StrictLogging { override protected def routes: Route = pathEndOrSingleSlash { get { complete(\u0026#34;Welcome!\u0026#34;) } } } You can test it in the console:\n$ ~ curl --request GET --url http://localhost:8080 Welcome! Basic Auth Next, we\u0026rsquo;ll add the auth endpoint:\npath(\u0026#34;auth\u0026#34;) { authenticateBasic(realm = \u0026#34;auth\u0026#34;, BasicAuthAuthenticator) { user =\u0026gt; post { // do stuff with user  } } } This calls a BasicAuthAuthenticator function which goes through the list of validBasicAuthCredentials and compares username and password. Note that p.verify(user.password) does a constant time comparison to make this invulnerable against timing attacks:\ncase class BasicAuthCredentials(username: String, password: String) private val validBasicAuthCredentials = Seq(BasicAuthCredentials(\u0026#34;jannik\u0026#34;, \u0026#34;p4ssw0rd\u0026#34;)) private def BasicAuthAuthenticator(credentials: Credentials) = credentials match { case p @ Credentials.Provided(_) =\u0026gt; validBasicAuthCredentials .find(user =\u0026gt; user.username == p.identifier \u0026amp;\u0026amp; p.verify(user.password)) case _ =\u0026gt; None } Our code then creates an access_token, stores it in a list and returns it:\nprivate val loggedInUsers = mutable.ArrayBuffer.empty[LoggedInUser] case class oAuthToken(access_token: String = java.util.UUID.randomUUID().toString, token_type: String = \u0026#34;bearer\u0026#34;, expires_in: Int = 3600) case class LoggedInUser(basicAuthCredentials: BasicAuthCredentials, oAuthToken: oAuthToken = new oAuthToken, loggedInAt: LocalDateTime = LocalDateTime.now()) Inside the /auth route:\npath(\u0026#34;auth\u0026#34;) { authenticateBasic(realm = \u0026#34;auth\u0026#34;, BasicAuthAuthenticator) { user =\u0026gt; post { val loggedInUser = LoggedInUser(user) loggedInUsers.append(loggedInUser) complete(loggedInUser.oAuthToken) } } } To make this work with JSON, I also added the following lines:\nimport de.heikoseeberger.akkahttpjson4s.Json4sSupport._ implicit val formats: DefaultFormats.type = DefaultFormats implicit val serialization: Serialization.type = native.Serialization Try getting an access_token via\n$ curl --request POST --url http://localhost:8080/auth --header \u0026#39;authorization: Basic amFubmlrOnA0c3N3MHJk\u0026#39; {\u0026#34;access_token\u0026#34;:\u0026#34;2e510027-0eb9-4367-b310-68e1bab9dc3d\u0026#34;, \u0026#34;token_type\u0026#34;:\u0026#34;bearer\u0026#34;, \u0026#34;expires_in\u0026#34;:3600} oAuth The /api endpoint looks very similar:\npath(\u0026#34;api\u0026#34;) { authenticateOAuth2(realm = \u0026#34;api\u0026#34;, oAuthAuthenticator) { validToken =\u0026gt; complete(s\u0026#34;It worked! user = $validToken\u0026#34;) } } It calls the oAuthAuthenticator function which looks through the list of loggedInUsers.\nprivate def oAuthAuthenticator(credentials: Credentials): Option[LoggedInUser] = credentials match { case p @ Credentials.Provided(_) =\u0026gt; loggedInUsers.find(user =\u0026gt; p.verify(user.oAuthToken.access_token)) case _ =\u0026gt; None } You call this endpoint via\n$ curl --request GET --url http://localhost:8080/api --header \u0026#39;authorization: Bearer 2e510027-0eb9-4367-b310-68e1bab9dc3d\u0026#39; \u0026#34;It worked! user = LoggedInUser(BasicAuthCredentials(jannik,p4ssw0rd),oAuthToken(2e510027-0eb9-4367-b310-68e1bab9dc3d,bearer,3600),2018-10-28T12:58:33.048)\u0026#34; Expire Sessions For the final touches, we can hook into the ActorSystem to schedule a cleanUpExpiredUsers() function:\noverride def postHttpBinding(binding: Http.ServerBinding): Unit = { systemReference.get().scheduler.schedule(5 minutes, 5 minutes)(cleanUpExpiredUsers())(systemReference.get().dispatcher) super.postHttpBinding(binding) } private def cleanUpExpiredUsers(): Unit = loggedInUsers .filter(user =\u0026gt; user.loggedInAt .plusSeconds(user.oAuthToken.expires_in) .isBefore(LocalDateTime.now())) .foreach(loggedInUsers -= _) If you look inside the HttpApp you\u0026rsquo;ll notice that the ActorSystem isn\u0026rsquo;t initialized until startServer() is called. Therefore we can only access it afterwards. This is easily done by overriding postHttpBinding().\nSince we store the time of login, we can simply add the expires_in to that an check if that LocalDateTime is in the past. If so, the session has expired and we remove the user from the loggedInUsers list.\nComplete Example And now the complete example:\nimport java.time.LocalDateTime import akka.http.scaladsl.Http import akka.http.scaladsl.server._ import akka.http.scaladsl.server.directives.Credentials import com.typesafe.scalalogging.StrictLogging import org.json4s.native.Serialization import org.json4s.{DefaultFormats, native} import scala.collection.mutable import scala.concurrent.duration._ import scala.language.postfixOps object Main { def main(args: Array[String]): Unit = { val port: Int = sys.env.getOrElse(\u0026#34;PORT\u0026#34;, \u0026#34;8080\u0026#34;).toInt WebServer.startServer(\u0026#34;0.0.0.0\u0026#34;, port) } } object WebServer extends HttpApp with StrictLogging { import de.heikoseeberger.akkahttpjson4s.Json4sSupport._ implicit val formats: DefaultFormats.type = DefaultFormats implicit val serialization: Serialization.type = native.Serialization // TODO load from external source  private val validBasicAuthCredentials = Seq(BasicAuthCredentials(\u0026#34;jannik\u0026#34;, \u0026#34;p4ssw0rd\u0026#34;)) // TODO persist to make sessions survive restarts  private val loggedInUsers = mutable.ArrayBuffer.empty[LoggedInUser] override def postHttpBinding(binding: Http.ServerBinding): Unit = { systemReference.get().scheduler.schedule(5 minutes, 5 minutes)(cleanUpExpiredUsers())(systemReference.get().dispatcher) super.postHttpBinding(binding) } override protected def routes: Route = pathEndOrSingleSlash { get { complete(\u0026#34;Welcome!\u0026#34;) } } ~ path(\u0026#34;auth\u0026#34;) { authenticateBasic(realm = \u0026#34;auth\u0026#34;, BasicAuthAuthenticator) { user =\u0026gt; post { val loggedInUser = LoggedInUser(user) loggedInUsers.append(loggedInUser) complete(loggedInUser.oAuthToken) } } } ~ path(\u0026#34;api\u0026#34;) { authenticateOAuth2(realm = \u0026#34;api\u0026#34;, oAuthAuthenticator) { validToken =\u0026gt; complete(s\u0026#34;It worked! user = $validToken\u0026#34;) } } private def BasicAuthAuthenticator(credentials: Credentials): Option[BasicAuthCredentials] = credentials match { case p @ Credentials.Provided(_) =\u0026gt; validBasicAuthCredentials.find(user =\u0026gt; user.username == p.identifier \u0026amp;\u0026amp; p.verify(user.password)) case _ =\u0026gt; None } private def oAuthAuthenticator(credentials: Credentials): Option[LoggedInUser] = credentials match { case p @ Credentials.Provided(_) =\u0026gt; loggedInUsers.find(user =\u0026gt; p.verify(user.oAuthToken.access_token)) case _ =\u0026gt; None } private def cleanUpExpiredUsers(): Unit = loggedInUsers .filter(user =\u0026gt; user.loggedInAt.plusSeconds(user.oAuthToken.expires_in).isBefore(LocalDateTime.now())) .foreach(loggedInUsers -= _) case class BasicAuthCredentials(username: String, password: String) case class oAuthToken(access_token: String = java.util.UUID.randomUUID().toString, token_type: String = \u0026#34;bearer\u0026#34;, expires_in: Int = 3600) case class LoggedInUser(basicAuthCredentials: BasicAuthCredentials, oAuthToken: oAuthToken = new oAuthToken, loggedInAt: LocalDateTime = LocalDateTime.now()) } The next steps would be to fill the validBasicAuthCredentials from somewhere outside the code and store the loggedInUsers outside the runtime to make sessions survive restarts.\n","permalink":"https://www.jannikarndt.de/blog/2018/10/oauth2-akka-http/","summary":"\u003cp\u003eThis is a basic example how to implement oAuth2 using Akka HTTP and Scala. It provides three endpoints. From the clients point of view:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/\u003c/code\u003e — publicly accessible, returns “Welcome!”,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/auth\u003c/code\u003e — provide your \u003ccode\u003eusername\u003c/code\u003e and \u003ccode\u003epassword\u003c/code\u003e, receive an \u003ccode\u003eaccess_token\u003c/code\u003e in return,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/api\u003c/code\u003e — secured by oAuth, send the \u003ccode\u003eaccess_token\u003c/code\u003e in a header to gain access.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFrom the server\u0026rsquo;s point of view:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/\u003c/code\u003e — publicly accessible, do nothing,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/auth\u003c/code\u003e — receive basic auth credentials, verify they\u0026rsquo;re in the list of known credentials, create an \u003ccode\u003eaccess_token\u003c/code\u003e, return it,\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/api\u003c/code\u003e — receive \u003ccode\u003eauthorization\u003c/code\u003e header, check if \u003ccode\u003eaccess_token\u003c/code\u003e is in list of valid tokens.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSince oAuth tokens are short lived, the server also has to invalidate expired tokens.\u003c/p\u003e","title":"oAuth2 with Akka HTTP"},{"content":"Getting a Akka HTTP-based backend up and running on Heroku for free can be done in less then 30 minutes — if you know the tricks.\nSetup First, create a new app on Heroku. The easiest way to deploy something is to push it to the attached git repository, as explained on the Deploy page:\n\nNext, setup you local sbt project. My way would be\n$ sbt new jannikarndt/scala.g8 Do the git thing:\n$ git init $ git add . $ git commit -m \u0026#34;Template\u0026#34; and next, connect the repository to Heroku. If you haven\u0026rsquo;t already, install their cli first:\n$ brew install heroku/brew/heroku ... $ heroku login heroku: Enter your login credentials Email: your@mail.com Password: ************ Logged in as your@mail.com $ heroku git:remote -a yourproject set git remote heroku to https://git.heroku.com/yourproject.git plugins.sbt and build.sbt To create a build that can be run on Heroku, you need the sbt-native-packager in your project/plugins.sbt:\naddSbtPlugin(\u0026#34;com.typesafe.sbt\u0026#34; % \u0026#34;sbt-native-packager\u0026#34; % \u0026#34;1.3.12\u0026#34;) You also have to enable the plugin in the build.sbt:\nenablePlugins(JavaAppPackaging) This enables the stage command. Otherwise you\u0026rsquo;ll end up with weird errors:\nsbt\u0026gt; stage [error] Not a valid command: stage (similar: last-grep, set, last) [error] Not a valid project ID: stage [error] Expected \u0026#39;:\u0026#39; [error] Not a valid key: stage (similar: state, target, tags) [error] stage [error] ^ sbt\u0026gt; compile stage [error] Expected whitespace character [error] Expected \u0026#39;/\u0026#39; [error] compile stage [error] ^ Next, you need to define the Main class to be run. This is also done in your build.sbt:\nmainClass in Compile := Some(\u0026#34;Main\u0026#34;) Note: If you use packages, you\u0026rsquo;ll need to write the full identifier, i.e. com.mycompany.myproject.Main or something similar.\nIf you get this part wrong, sbt stage will warn you:\nsbt\u0026gt; stage [info] Wrote /target/scala-2.12/yourproject_2.12-1.0.pom [info] Packaging /target/scala-2.12/yourproject_2.12-1.0.jar ... [info] Done packaging. [warn] You have no main class in your project. No start script will be generated. [warn] You have no main class in your project. No start script will be generated. [success] Total time: 0 s, completed Oct 27, 2018 6:13:13 PM but on Heroku it will actually crash the app:\nheroku[web.1]: State changed from starting to crashed app[web.1]: Error: Main method not found in class Main, please define the main method as: app[web.1]: public static void main(String[] args) app[web.1]: or a JavaFX application class must extend javafx.application.Application The final build.sbt should look something like this:\nname := \u0026#34;yourproject\u0026#34; scalaVersion := \u0026#34;2.12.7\u0026#34; version := \u0026#34;1.0\u0026#34; maintainer := \u0026#34;you\u0026#34; mainClass in Compile := Some(\u0026#34;Main\u0026#34;) enablePlugins(JavaAppPackaging) libraryDependencies ++= Seq( \u0026#34;com.typesafe.akka\u0026#34; %% \u0026#34;akka-http\u0026#34; % \u0026#34;10.1.5\u0026#34;, \u0026#34;com.typesafe.akka\u0026#34; %% \u0026#34;akka-stream\u0026#34; % \u0026#34;2.5.17\u0026#34; ) Main.scala Now that you\u0026rsquo;ve managed to create a build and reference the correct mainClass, there\u0026rsquo;s only two caveats left: Binding the server. First, on Heroku you can not bind to localhost, you have to write 0.0.0.0 instead. Secondly, the port changes with every start of the app and is given as a parameter (-Dhttp.port=36803) and environment variable ($PORT). The code for this:\nval port: Int = sys.env.getOrElse(\u0026#34;PORT\u0026#34;, \u0026#34;8080\u0026#34;).toInt WebServer.startServer(\u0026#34;0.0.0.0\u0026#34;, port) A complete example for a WebServer would be\nimport akka.http.scaladsl.server._ object Main { def main(args: Array[String]): Unit = { val port: Int = sys.env.getOrElse(\u0026#34;PORT\u0026#34;, \u0026#34;8080\u0026#34;).toInt WebServer.startServer(\u0026#34;0.0.0.0\u0026#34;, port) } } object WebServer extends HttpApp { override protected def routes: Route = pathEndOrSingleSlash { get { complete(\u0026#34;It works\u0026#34;) } } } Deploy After you tested locally running\n$ sbt compile stage (because that\u0026rsquo;s exactly the command that will be run on Heroku\u0026rsquo;s server) and maybe sbt run or sbt reStart, you can finally deploy.\nSince you already connected our git to the remote repository, all you have to do is push:\n$ git push --set-upstream heroku master Enumerating objects: 13, done. Counting objects: 100% (13/13), done. Delta compression using up to 4 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (7/7), 1.25 KiB | 1.25 MiB/s, done. Total 7 (delta 3), reused 0 (delta 0) remote: Compressing source files... done. remote: Building source: remote: remote: -----\u0026gt; Scala app detected remote: -----\u0026gt; Installing JDK 1.8... done remote: -----\u0026gt; Running: sbt compile stage ... remote: [info] Done compiling. remote: [success] Total time: 4 s, completed Oct 27, 2018 5:44:33 PM ... remote: [info] Done packaging. remote: [success] Total time: 3 s, completed Oct 27, 2018 5:44:36 PM remote: -----\u0026gt; Dropping ivy cache from the slug remote: -----\u0026gt; Dropping sbt boot dir from the slug remote: -----\u0026gt; Dropping compilation artifacts from the slug remote: -----\u0026gt; Discovering process types remote: Procfile declares types -\u0026gt; (none) remote: Default types for buildpack -\u0026gt; web remote: remote: -----\u0026gt; Compressing... remote: Done: 71.9M remote: -----\u0026gt; Launching... remote: Released v10 remote: https://yourproject.herokuapp.com/ deployed to Heroku remote: remote: Verifying deploy... done. To https://git.heroku.com/yourproject.git ca5c7c5..4a5a138 master -\u0026gt; master Now head to your url and start building the rest!\n","permalink":"https://www.jannikarndt.de/blog/2018/10/akka_http_on_heroku/","summary":"\u003cp\u003eGetting a Akka HTTP-based backend up and running on Heroku for free can be done in less then 30 minutes — if you know the tricks.\u003c/p\u003e","title":"Akka HTTP on Heroku"},{"content":"This example show how to write a reactive reader for the AWS Simple Queue Service, using Scala and alpakka (respective akka streams).\nSQS Basics SQS is a AWS-managed message queue service. It can contain several queues. If a message is read from the queue, it is internally set to invisible for 30 seconds. If you don\u0026rsquo;t delete it after these 30 seconds, it becomes visible again. This is great for resilient, distributed microservices: If one instance of a service dies in the middle of handling a message, the message will re-appear and be handled by another instance. It is important though that you do the deletion step last.\nThis Project You can find the complete source code on https://github.com/JannikArndt/reactive-sqs.\nImports / build.sbt Let\u0026rsquo;s start with adding dependencies to our build.sbt:\nname := \u0026#34;reactive-sqs\u0026#34; scalaVersion := \u0026#34;2.12.7\u0026#34; version := \u0026#34;1.0\u0026#34; libraryDependencies ++= Seq( // https://mvnrepository.com/artifact/com.lightbend.akka/akka-stream-alpakka-sqs  \u0026#34;com.lightbend.akka\u0026#34; %% \u0026#34;akka-stream-alpakka-sqs\u0026#34; % \u0026#34;0.20\u0026#34;, // https://mvnrepository.com/artifact/com.typesafe.scala-logging/scala-logging  \u0026#34;com.typesafe.scala-logging\u0026#34; %% \u0026#34;scala-logging\u0026#34; % \u0026#34;3.9.0\u0026#34;, \u0026#34;org.slf4j\u0026#34; % \u0026#34;slf4j-simple\u0026#34; % \u0026#34;1.7.25\u0026#34;, // https://mvnrepository.com/artifact/com.typesafe.akka/akka-testkit  \u0026#34;com.typesafe.akka\u0026#34; %% \u0026#34;akka-testkit\u0026#34; % \u0026#34;2.5.16\u0026#34; % Test, // https://mvnrepository.com/artifact/org.scalatest/scalatest  \u0026#34;org.scalatest\u0026#34; %% \u0026#34;scalatest\u0026#34; % \u0026#34;3.0.5\u0026#34; % Test, // https://mvnrepository.com/artifact/org.mockito/mockito-core  \u0026#34;org.mockito\u0026#34; % \u0026#34;mockito-core\u0026#34; % \u0026#34;2.23.0\u0026#34; ) Since the com.amazonaws.aws-java-sdk-sqs-library is included in akka-stream-alpakka-sqs, we only need that one. I\u0026rsquo;m also using scala-logging, and for tests the akka-testkit, scalatest and mockito.\nSqsService.scala Next we\u0026rsquo;ll write the SqsService.scala. Its job is to create a flow and handle the messages. As parameters I want to give it the queueUrl, a function to handle messages and the maximum amount of messages that are handled in parallel:\nobject SqsService { case class MyMessage(content: String) def create(queueUrl: String, maxMessagesInFlight: Int) (messageHandler: MyMessage =\u0026gt; Unit) = ??? } I can call this function from my Main.scala:\nSqsService.create(\u0026#34;http://localhost:4576/queue/myqueue\u0026#34;, 20) { message =\u0026gt; println(s\u0026#34;Doing logic with ${message.content}\u0026#34;) } Since alpakka is running on akka streams, I will also have to provide an ActorSystem(). The return value of my create function is dictated by akka streams: a tuple of a KillSwitch and a Future[Done]. These enable me to stop the stream and wait for its completion. So the complete Main is\nimport akka.Done import akka.actor.ActorSystem import akka.stream._ import scala.concurrent.duration._ import scala.concurrent.{Await, Future} import scala.io.StdIn import scala.language.postfixOps object Main extends App { implicit val system = ActorSystem() val (killSwitch, completion): (KillSwitch, Future[Done]) = SqsService.create(\u0026#34;http://localhost:4576/queue/myqueue\u0026#34;, 20) { message =\u0026gt; println(s\u0026#34;Doing logic with ${message.content}\u0026#34;) } println(s\u0026#34;Running service. Press enter to stop.\u0026#34;) StdIn.readLine() killSwitch.shutdown() Await.ready(completion, 10 seconds) SqsService.stop() system.terminate() } Creating an SqsClient The first thing our SqsService needs to do is create an AmazonSQSAsyncClient. The last thing it needs to do is to shut it down — otherwise it won\u0026rsquo;t let you exit the program:\nobject SqsService extends StrictLogging { case class MyMessage(content: String) implicit private val sqsClient: AmazonSQSAsync = AmazonSQSAsyncClientBuilder .standard() .withRegion(\u0026#34;eu-central-1\u0026#34;) .build() def stop(): Unit = sqsClient.shutdown() def create(queueUrl: String, maxMessagesInFlight: Int) (messageHandler: MyMessage =\u0026gt; Unit) = ??? } Creating the Flow Next, we\u0026rsquo;ll implement the create function to create a Flow. The basic stream is build by this:\nsource .via(flow) .toMat(sink)(Keep.both) .run() As source we\u0026rsquo;ll use\nSqsSource(queueUrl).viaMat(KillSwitches.single)(Keep.right) This combines a SqsSource that emits sqs.model.Messages with a KillSwitch and makes sure the KillSwitch is returned. The return type is Source[Message, UniqueKillSwitch].\nAs sink we use SqsAckSink(queueUrl, SqsAckSinkSettings(maxMessagesInFlight)). It needs to receive the queueUrl because that\u0026rsquo;s where it sends the delete-commands to delete/acknowledge the currently invisible message.\nThe Flow is created in two steps: Step one creates a flow from a function and defines attributes:\nval flow = Flow .fromFunction(handleMessage(messageHandler)) .withAttributes(ActorAttributes.supervisionStrategy(Supervision.resumingDecider)) The supervisionStrategy decides what happens if an exception is thrown inside the flow. The standard strategy is to complete the stream with failure, i.e. one bad message will crash the entire program. The resumingDecider simply ignores elements that result in exceptions. This means you need to implement error handling inside the messageHandler.\nThe function itself reads the body from the message and hands it to the messageHandler we defined in the Main. It then returns a tuple of the original message and a delete-action. This is the input needed by the SqsAckSink:\nprivate def handleMessage(messageHandler: MyMessage =\u0026gt; Unit) = { message: Message =\u0026gt; messageHandler(MyMessage(message.getBody)) (message, MessageAction.Delete) } Now there\u0026rsquo;s only one thing left: The run() part, where the stream is materialized, needs an ActorMaterializer:\nimplicit val mat: ActorMaterializer = ActorMaterializer() The complete code is\nobject SqsService extends StrictLogging { case class MyMessage(content: String) implicit private val sqsClient: AmazonSQSAsync = AmazonSQSAsyncClientBuilder .standard() .withRegion(\u0026#34;eu-central-1\u0026#34;) .build() def stop(): Unit = sqsClient.shutdown() def create(queueUrl: String, maxMessagesInFlight: Int) (messageHandler: MyMessage =\u0026gt; Unit) (implicit system: ActorSystem): (KillSwitch, Future[Done]) = { implicit val mat: ActorMaterializer = ActorMaterializer() val source = SqsSource(queueUrl).viaMat(KillSwitches.single)(Keep.right) val sink = SqsAckSink(queueUrl, SqsAckSinkSettings(maxMessagesInFlight)) val flow = Flow .fromFunction(handleMessage(messageHandler)) .withAttributes(ActorAttributes.supervisionStrategy(Supervision.resumingDecider)) source .via(flow) .toMat(sink)(Keep.both) .run() } private def handleMessage(messageHandler: MyMessage =\u0026gt; Unit) = { message: Message =\u0026gt; messageHandler(MyMessage(message.getBody)) (message, MessageAction.Delete) } } Bonus Functions The AmazonSqsClient has one big caveat: it does not fail if the queue you\u0026rsquo;re subscribing to doesn\u0026rsquo;t exist. That\u0026rsquo;s why I wrote two extra functions:\nimport scala.collection.JavaConverters._ def findAvailableQueues(queueNamePrefix: String): Seq[String] = sqsClient.listQueues(queueNamePrefix).getQueueUrls.asScala.toVector This is just a Scala-wrapper around the library function. The second function calls a library function that does fail if the queue doesn\u0026rsquo;t exist:\ndef assertQueueExists(queueUrl: String): Unit = try { sqsClient.getQueueAttributes(queueUrl, Seq(\u0026#34;All\u0026#34;).asJava) logger.info(s\u0026#34;Queue at $queueUrlfound.\u0026#34;) } catch { case queueDoesNotExistException: QueueDoesNotExistException =\u0026gt; logger.error(s\u0026#34;The queue with url $queueUrldoes not exist.\u0026#34;) throw queueDoesNotExistException } The advantage of assertQueueExists over checking if the url is contained in the list of all available queues is, that you don\u0026rsquo;t need the permission to list all queues.\nTesting Testing our SqsService has three challenges: It is using an AWS service, it is running asynchronously and message queues have a live of their own.\nMocking AWS SQS Luckily, others have had the need to test AWS services as well, and created Localstack. It provides a Docker image that runs these services locally:\n$ docker run -d --env SERVICES=\u0026#34;sqs\u0026#34; --env TMPDIR=\u0026#34;/tmp\u0026#34; \\  --name \u0026#34;localstack\u0026#34; \\  --publish 4576:4576 \\  --rm localstack/localstack You list the services you want to use in the SERVICES variable and expose their respective port (--publish). The container is started in detached mode (-d) and cleans up after it is removed or the daemon exits (--rm).\nIf you want to access the Localstack-version of a service via the aws cli, you can use the --endpoint-url:\n$ aws --endpoint-url=http://localhost:4576 sqs send-message\\  --queue-url \u0026#34;http://localhost:4576/queue/myqueue\u0026#34;\\  --message-body \u0026#34;Hallo\u0026#34; Mocking the Function Our test will basically be “is this function called if a message arrives in SQS?”. For this we need\n the Localstack-version of SQS running a seperate SQSClient a queue dedicated for this test a way to test if a function gets called a way to wait for the round trip (test =\u0026gt; Sqs =\u0026gt; tested code =\u0026gt; test)  We use the AWS client library thats included in alpakka to create a client in our test class:\nval awsSqsClient: AmazonSQSAsync = AmazonSQSAsyncClientBuilder .standard() .withEndpointConfiguration(new EndpointConfiguration(\u0026#34;http://localhost:4576\u0026#34;, \u0026#34;eu-central-1\u0026#34;)) .build() Note that the endpoint connects to the Localstack-version running in docker.\nNext, we\u0026rsquo;ll create a queue:\nval queueUrl: String = awsSqsClient.createQueue(\u0026#34;integrationtest\u0026#34;).getQueueUrl Since we\u0026rsquo;re running on akka, with it\u0026rsquo;s own ExecutionContext, and the SqsClient has it\u0026rsquo;s own ExecutionContext as well, we should terminate both when the tests are done:\noverride def afterAll(): Unit = { awsSqsClient.shutdown() shutdown(system) super.afterAll() } We\u0026rsquo;ll use Mockito to verify that the function we\u0026rsquo;ll give to our SqsService is actually called. Mockito needs a class to create a mock, so:\nclass TestClass { def testFunction(message: MyMessage): Unit = Unit } val mock: TestClass = mock[TestClass] Since we\u0026rsquo;re mocking the testFunction, we don\u0026rsquo;t need to implement it.\nIt is good practice to put any values that appear in the test input as well as the output into a variable, so you can easily spot the expectation:\nval messageBody = \u0026#34;Example Body\u0026#34; Now we\u0026rsquo;re ready to write the test!\n\u0026#34;SqsService\u0026#34; should \u0026#34;receive a message\u0026#34; in { // Arrange  SqsService.create(queueUrl, maxMessagesInFlight = 20)(mock.testFunction) // Act: Send message to SQS (synchronous)  awsSqsClient.sendMessage(queueUrl, messageBody) // Assert  verify(mock, Mockito.timeout(1000)).testFunction(MyMessage(messageBody)) } The Mockito.timeout(1000) will wait a second for the result. Make sure to use timeout instead of after, because with timeout the test succeeds directly when the function is called, while after waits the full second.\nDealing with Message Queues Now the tests will mostly work. However, since it depends on an outside component, namely the SQS, it will fail, from time to time.\n[info] MainSpec: [info] SqsService [info] - should receive a message *** FAILED *** [info] org.mockito.exceptions.verification.WantedButNotInvoked: Wanted but not invoked: [info] testClass.testFunction( [info] MyMessage(Example Body) [info] ); [info] -\u0026gt; at MainSpec.$anonfun$new$1(MainSpec.scala:64) It might also fail because the round trip takes longer on you CI-server or you broke the code. Try setting the timeout to around 100 ms to replicate the behavior. Now what\u0026rsquo;s really bad is that your next test will fail as well:\n[info] MainSpec: [info] SqsService [info] - should receive a message *** FAILED *** [info] org.mockito.exceptions.verification.TooManyActualInvocations: testClass.testFunction( [info] MyMessage(Example Body) [info] ); [info] Wanted 1 time: [info] -\u0026gt; at MainSpec.$anonfun$new$1(MainSpec.scala:64) [info] But was 2 times: [info] -\u0026gt; at MainSpec.$anonfun$new$2(MainSpec.scala:58) [info] -\u0026gt; at MainSpec.$anonfun$new$2(MainSpec.scala:58) That\u0026rsquo;s because the message from the previous test is still in the queue! Luckily, this can be worked around with BeforeAndAfterEach:\nvar queueUrl: String = \u0026#34;\u0026#34; override def beforeEach(): Unit = { queueUrl = awsSqsClient.createQueue(\u0026#34;integrationtest\u0026#34;).getQueueUrl println(\u0026#34;--- Created queue ---\u0026#34;) super.beforeEach() } override def afterEach(): Unit = { awsSqsClient.deleteQueue(queueUrl) println(\u0026#34;--- Deleted queue ---\u0026#34;) super.afterEach() } This way, one failed test won\u0026rsquo;t affect the next.\n","permalink":"https://www.jannikarndt.de/blog/2018/10/reactive_sqs_reader/","summary":"\u003cp\u003eThis example show how to write a reactive reader for the AWS \u003cem\u003eSimple Queue Service\u003c/em\u003e, using Scala and alpakka (respective akka streams).\u003c/p\u003e","title":"How to Write and Test a Reactive Reader for AWS SQS Using akka, alpakka and Localstack"},{"content":"While the Akka documentation is incredibly well written, it has surprisingly few images. Since I visualize concepts to remember them, here is my take on how Event Sourcing in Akka Persistence works:\n\nThe text I took this from is\n A persistent actor receives a (non-persistent) command which is first validated if it can be applied to the current state. Here validation can mean anything, from simple inspection of a command message’s fields up to a conversation with several external services, for example. If validation succeeds, events are generated from the command, representing the effect of the command. These events are then persisted and, after successful persistence, used to change the actor’s state. When the persistent actor needs to be recovered, only the persisted events are replayed of which we know that they can be successfully applied. In other words, events cannot fail when being replayed to a persistent actor, in contrast to commands. Event sourced actors may also process commands that do not change application state such as query commands for example.\n https://doc.akka.io/docs/akka/2.5/persistence.html#event-sourcing, emphasis mine\n","permalink":"https://www.jannikarndt.de/blog/2018/08/event_sourcing_in_akka_persistent_actors/","summary":"\u003cp\u003eWhile the \u003ca href=\"https://doc.akka.io/docs/\"\u003eAkka\u003c/a\u003e documentation is incredibly well written, it has surprisingly few images. Since \u003cem\u003eI\u003c/em\u003e visualize concepts to remember them, here is my take on how Event Sourcing in Akka Persistence works:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2018/08/AkkaPersistence.svg\"\u003e\u003cimg src=\"/blog/2018/08/AkkaPersistence.svg\" alt=\"\" onerror=\"this.onerror=null; this.src='/blog/2018/08/AkkaPersistence.png'\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"How Event Sourcing in Akka Persistent Actors Works"},{"content":"Changing the password for a PostgreSQL database user involves two steps: The change in the database and the change in the application code. This blog post describes how to do this without any downtime or failed authentication tries.\nThree Scenarios There are basically three scenarios how you could handle a credential change:\n Update the database, then update the application. In between you\u0026rsquo;ll have a short time when the application can\u0026rsquo;t connect. Make both the old and new password known to the application and let it fall back on the new one once the old one fails. Create a copy of the database user, update the application, delete the old user.  Scenario 1 is not an option for service accounts, and scenario 2 is complicated and still triggers your alarms for unsuccessful login attempts (you have one, don\u0026rsquo;t you?). While scenario 3 sounds complicated at first, it is very easy to achieve in PostgreSQL.\nThree Users First of all: PostgreSQL doesn\u0026rsquo;t have users, it has roles. The difference: Without explicitly stating it, you cannot use a role to log in. First, you have to define it to be WITH LOGIN WITH PASSWORD. Also, roles can assume other roles.\nThe solution to our problem thus is to have three roles:\n One that has all the permissions and objects attached to it (main role), One that is used to log in and assume the first role (active sub-role), One that is not yet used to log in and assume the first role (inactive sub-role).  The SQL statement to create these roles is\nCREATE ROLE my_app WITH NOLOGIN; CREATE ROLE my_app_tom WITH NOLOGIN IN ROLE my_app; ALTER ROLE my_app_tom SET ROLE TO my_app; CREATE ROLE my_app_jerry WITH NOLOGIN IN ROLE my_app; ALTER ROLE my_app_jerry SET ROLE TO my_app; All GRANTS referenec the main role, my_app, and are inherited by the sub-roles. Also, the SET ROLE make the role assume the main role on login. Next, you allow one of the sub-roles to log in:\nALTER ROLE my_app_tom WITH LOGIN; ALTER ROLE my_app_tom WITH PASSWORD \u0026#39;mySuperSecretPassword\u0026#39;; Now if you want to rotate your credentials, simply change the inactive sub-role to be active as well:\nALTER ROLE my_app_jerry WITH LOGIN; ALTER ROLE my_app_jerry WITH PASSWORD \u0026#39;myNEWSuperSecretPassword\u0026#39;; change the application and then deactivate the old role:\nALTER ROLE my_app_tom WITH NOLOGIN; Using Liquibase If you use Liquibase to manage your database, you might be tempted to handle users directly, since it\u0026rsquo;s not supported in their schema. But there\u0026rsquo;s no need to, the following will work jsut fine:\n\u0026lt;databaseChangeLog xmlns=\u0026#34;http://www.liquibase.org/xml/ns/dbchangelog\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-3.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- Load passwords into properties --\u0026gt; \u0026lt;include file=\u0026#34;passwords.xml\u0026#34;/\u0026gt; \u0026lt;!-- Create accounts --\u0026gt; \u0026lt;changeSet id=\u0026#34;create_account_my_app\u0026#34; author=\u0026#34;jannik.arndt@holisticon.de\u0026#34;\u0026gt; \u0026lt;sql\u0026gt; CREATE ROLE my_app WITH NOLOGIN; CREATE ROLE my_app_tom WITH NOLOGIN IN ROLE my_app; ALTER ROLE my_app_tom SET ROLE TO my_app; CREATE ROLE my_app_jerry WITH NOLOGIN IN ROLE my_app; ALTER ROLE my_app_jerry SET ROLE TO my_app; \u0026lt;/sql\u0026gt; \u0026lt;rollback\u0026gt; DROP ROLE my_app_jerry, my_app_tom; REVOKE ALL ON SCHEMA my_schema FROM my_app; REVOKE ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA my_schema FROM my_app; REVOKE USAGE ON SCHEMA my_schema FROM my_app; REASSIGN OWNED BY my_app TO postgres; DROP OWNED BY my_app; -- for privileges DROP ROLE my_app; \u0026lt;/rollback\u0026gt; \u0026lt;/changeSet\u0026gt; \u0026lt;!-- Set active role --\u0026gt; \u0026lt;changeSet id=\u0026#34;allow_login_for_my_app\u0026#34; author=\u0026#34;jannik.arndt@holisticon.de\u0026#34; runOnChange=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;sql\u0026gt; ALTER ROLE my_app_tom WITH LOGIN; ALTER ROLE my_app_jerry WITH NOLOGIN; \u0026lt;/sql\u0026gt; \u0026lt;/changeSet\u0026gt; \u0026lt;!-- Set Permissions / Grants --\u0026gt; \u0026lt;changeSet id=\u0026#34;grant_permissions_to_my_app\u0026#34; author=\u0026#34;jannik.arndt@holisticon.de\u0026#34; runOnChange=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;sqlFile path=\u0026#34;my_app_grants.sql\u0026#34;/\u0026gt; \u0026lt;/changeSet\u0026gt; \u0026lt;!-- Set Passwords --\u0026gt; \u0026lt;changeSet id=\u0026#34;set_passwords_dev\u0026#34; author=\u0026#34;jannik.arndt@holisticon.de\u0026#34; context=\u0026#34;dev or local\u0026#34; runOnChange=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;sql\u0026gt; ALTER ROLE my_app_tom WITH PASSWORD \u0026#39;${my_app_tom.password.dev}\u0026#39;; ALTER ROLE my_app_jerry WITH PASSWORD \u0026#39;${my_app_jerry.password.dev}\u0026#39;; \u0026lt;/sql\u0026gt; \u0026lt;rollback/\u0026gt; \u0026lt;/changeSet\u0026gt; \u0026lt;changeSet id=\u0026#34;set_passwords_prod\u0026#34; author=\u0026#34;jannik.arndt@holisticon.de\u0026#34; context=\u0026#34;prod\u0026#34; runOnChange=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;sql\u0026gt; ALTER ROLE my_app_tom WITH PASSWORD \u0026#39;${my_app_tom.password.prod}\u0026#39;; ALTER ROLE my_app_jerry WITH PASSWORD \u0026#39;${my_app_jerry.password.prod}\u0026#39;; \u0026lt;/sql\u0026gt; \u0026lt;rollback/\u0026gt; \u0026lt;/changeSet\u0026gt; \u0026lt;/databaseChangeLog\u0026gt; and passwords.xml lists:\n\u0026lt;databaseChangeLog xmlns=\u0026#34;http://www.liquibase.org/xml/ns/dbchangelog\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.liquibase.org/xml/ns/dbchangelog http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-3.0.xsd\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;my_app_tom.password.dev\u0026#34; value=\u0026#34;mySuperSecretPassword\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;my_app_tom.password.prod\u0026#34; value=\u0026#34;mySuperSecretPasswordForProd\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;my_app_jerry.password.dev\u0026#34; value=\u0026#34;myNEWSuperSecretPassword\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;my_app_jerry.password.prod\u0026#34; value=\u0026#34;myNEWSuperSecretPasswordForProd\u0026#34;/\u0026gt; \u0026lt;/databaseChangeLog\u0026gt; This does a few additional things:\nLiquibase generates a hash for every changeset and warns you, if a hash changed. Since the WITH LOGIN and WITH NOLOGIN attributes are supposed to change, we add the runOnChange attribute, which just runs the SQL command again. Note that this only works with idempotent commands, like ALTER. Running a CREATE ROLE twice would result in an error.\nTo automatically have different passwords for prod and dev (and local), we use the context attribute. This is evaluated if you add --contexts=prod to the command line when running liquibase. Note that if you don\u0026rsquo;t provide the command line argument, the changeset will not be run at all.\nAnd lastly, the passwords are loaded from an external file, so you can make sure that this is either on the .gitignore list or encrypted (e.g. via git-crypt).\n","permalink":"https://www.jannikarndt.de/blog/2018/08/rotating_postgresql_passwords_with_no_downtime/","summary":"\u003cp\u003eChanging the password for a PostgreSQL database user involves two steps: The change in the database and the change in the application code. This blog post describes how to do this without any downtime or failed authentication tries.\u003c/p\u003e","title":"Rotating PostgreSQL Passwords with no downtime"},{"content":"The PostgreSQL installation comes with a great tool, psql, to administer and inspect the database. pgcli extends this with syntax highlighting and autocompletion.\nStarting and Connecting psql comes with you local PostgreSQL installation, pgcli can be installed via brew install pgcli or pip install pgcli.\nWhen you start the tool, you need to specify the connection. PostgreSQL can be accessed via TCP/IP or Unix Sockets. If you installed Postgres.app, TCP will be the standard and you only need to specify the host (h):\n$ psql -h localhost This assumes three things:\n the user is your OS user the database name is the same as the user connections from localhost are automatically trusted the port is 5432  Let\u0026rsquo;s go into details here:\nThe User To be precise, PostgreSQL doesn\u0026rsquo;t have users, it has roles. Roles you can login with are created as ROLE WITH LOGIN. A standard installation usually comes with a role for the OS account and a postgres role. To add more roles, you need to login first.\n Tip: Users can be listed with the \\du+ command!\n The Database The OS user and the postgres role have a corresponding database, which is automatically selected. However, if you create a new role and try to log in as that role, you will likely receive the error\n$ psql -h localhost -p 5432 -U foo FATAL: database \u0026#34;foo\u0026#34; does not exist Thus you\u0026rsquo;ll have to specify the database you want to connect to as well:\n$ psql -h localhost -p 5432 -U foo -d postgres By the way: During a session you can always change the database you\u0026rsquo;re connected to with \\c \u0026lt;databasename\u0026gt;, but you cannot be connected to no database at any moment.\nThe Password In a fresh installation, your OS user and the postgres role do not have a password set. This would usually mean that you cannot use that role to login with, but the standard Client Authentication Configuration, i.e. the pg_hba.conf file is configured to trust all connections from localhost or 127.0.0.1/32.\nActually assigning a password to a role won\u0026rsquo;t change anything, only if you change the line in the pg_hba.conf too will you be asked to provide a password when logging in:\n# TYPE DATABASE USER ADDRESS METHOD # IPv4 local connections: host all all 127.0.0.1/32 md5 # IPv6 local connections: host all all ::1/128 md5  Tip: Starting with PostgreSQL 10, you can also use scram-sha-256 for password hashing.\n Since you don\u0026rsquo;t want to pass the password as a command line parameter, since it is then leaked into the history-file, you can add an entry to a .pgpass file in your $HOME:\n# hostname:port:database:username:password localhost:5432:*:postgres:mySuperSecretPassword Now everytime you connect to a PostgreSQL instance on localhost port 5432 and any database using the postgres role, the password is filled in automatically.\nThe Port Since you can run multiple PostgreSQL instances on the same machine but all are reached via localhost, you need to provide different ports for every instance. If oyu only have one instance, the standard port is 5432.\nSumming up So in essence, the command\n$ psql -h localhost does the same as\n$ psql -h localhost -p 5432 -U \u0026lt;os-user\u0026gt; -d \u0026lt;os-user\u0026gt; Password for user postgres: ... Looking around While you can execute any SQL statement you want using the psql tool, there are a few shortcuts that make your life easier (if you can remember them). To list them all, write \\? or have a look at this cheat sheet:\nDatabases  Tip: If you don\u0026rsquo;t have a database to play with, try out this example dump!\n I like to start listing all databases in the current instance, using\njannikarndt@[local]:jannikarndt=# \\l+ List of databases ┌─────────────┬─────────────┬─────┬─────────┬────────────┬──────────────┐ │ Name │ Owner │ ... │ Size │ Tablespace │ Description │ ├─────────────┼─────────────┼─────┼─────────┼────────────┼──────────────┤ │ jannikarndt │ jannikarndt │ ... │ 7709 kB │ pg_default │ ... │ │ mywebshop │ jannikarndt │ ... │ 14 MB │ pg_default │ ... │ │ postgres │ postgres │ ... │ 8005 kB │ pg_default │ ... │ │ template0 │ postgres │ ... │ 7577 kB │ pg_default │ ... │ │ template1 │ postgres │ ... │ 7577 kB │ pg_default │ ... │ └─────────────┴─────────────┴─────┴─────────┴────────────┴──────────────┘ (5 rows) (slightly abbreviated by the columns Encoding, Collate, Ctype, Access privileges and Tablespace)\nAs you see, one of the databases is larger then the others. That might be an interesting one.\nTo list tables in that database, we first have to connect to it:\njannikarndt@[local]:jannikarndt=# \\dn+ List of schemas ┌────────┬──────────┬──────────────────────┬────────────────────────┐ │ Name │ Owner │ Access privileges │ Description │ ├────────┼──────────┼──────────────────────┼────────────────────────┤ │ public │ postgres │ postgres=UC/postgres↵│ standard public schema │ │ │ │ =UC/postgres │ │ └────────┴──────────┴──────────────────────┴────────────────────────┘ (1 row) jannikarndt@[local]:jannikarndt=# \\c mywebshop You are now connected to database \u0026#34;mywebshop\u0026#34; as user \u0026#34;jannikarndt\u0026#34;. jannikarndt@[local]:mywebshop=# \\dn+ List of schemas ┌─────────┬──────────┬──────────────────────┬────────────────────────┐ │ Name │ Owner │ Access privileges │ Description │ ├─────────┼──────────┼──────────────────────┼────────────────────────┤ │ public │ postgres │ postgres=UC/postgres↵│ standard public schema │ │ │ │ =UC/postgres │ │ │ webshop │ postgres │ │ │ └─────────┴──────────┴──────────────────────┴────────────────────────┘ (2 rows) Note that the first and the second entry for public are different schemas! The first is jannikarndt.public, the second is mywebshop.public.\nTables Next, let\u0026rsquo;s list all tables in the webshop schema. Running \\dt+ unfortunately will either result in\njannikarndt@[local]:mywebshop=# \\dt+ Did not find any relations. or showing tables that have nothing to do with the webshop schema:\njannikarndt@[local]:mywebshop=# \\dt+ List of relations ┌────────┬──────────────┬───────┬──────────┬─────────┬─────────────┐ │ Schema │ Name │ Type │ Owner │ Size │ Description │ ├────────┼──────────────┼───────┼──────────┼─────────┼─────────────┤ │ public │ public_table │ table │ postgres │ 0 bytes │ │ └────────┴──────────────┴───────┴──────────┴─────────┴─────────────┘ (1 row) This is because if you don\u0026rsquo;t enter a search path, psql will use your current search path, which points to the schema with your username and the public schema:\njannikarndt@[local]:mywebshop=# SHOW search_path mywebshop-# ; ┌─────────────────┐ │ search_path │ ├─────────────────┤ │ \u0026#34;$user\u0026#34;, public │ └─────────────────┘ (1 row) To list the tables of the webshop schema, you have to either set the search path using SET search_path TO webshop; or make it explicit in the query:\njannikarndt@[local]:mywebshop=# \\dt+ webshop.* List of relations ┌─────────┬─────────────────┬───────┬──────────┬─────────┬────────────────────┐ │ Schema │ Name │ Type │ Owner │ Size │ Description │ ├─────────┼─────────────────┼───────┼──────────┼─────────┼────────────────────┤ │ webshop │ address │ table │ postgres │ 136 kB │ Addresses for r... │ │ webshop │ articles │ table │ postgres │ 2872 kB │ Instance of a p... │ │ webshop │ colors │ table │ postgres │ 48 kB │ Colors with nam... │ │ webshop │ customer │ table │ postgres │ 152 kB │ Basic customer ... │ │ webshop │ labels │ table │ postgres │ 112 kB │ Brands / labels... │ │ webshop │ order │ table │ postgres │ 176 kB │ Metadata for an... │ │ webshop │ order_positions │ table │ postgres │ 384 kB │ Articles that a... │ │ webshop │ products │ table │ postgres │ 112 kB │ Groups articles... │ │ webshop │ sizes │ table │ postgres │ 16 kB │ Colors with nam... │ │ webshop │ stock │ table │ postgres │ 928 kB │ Amount of artic... │ └─────────┴─────────────────┴───────┴──────────┴─────────┴────────────────────┘ (10 rows) Setting the search path allows you to query multiple schemas:\njannikarndt@[local]:mywebshop=# SET search_path TO webshop,public; SET jannikarndt@[local]:mywebshop=# \\dt List of relations ┌─────────┬─────────────────┬───────┬──────────┐ │ Schema │ Name │ Type │ Owner │ ├─────────┼─────────────────┼───────┼──────────┤ │ public │ public_table │ table │ postgres │ │ webshop │ address │ table │ postgres │ │ ... │ ... │ ... │ ... │ └─────────┴─────────────────┴───────┴──────────┘ (11 rows) To query all schemas, run \\dt+ *.*, but beware, this also lists the tables in information_schema and pg_catalog, so… a lot.\nViews Attention: \\dt only lists tables, not views. Those need to be queried seperately:\njannikarndt@[local]:mywebshop=# \\dv+ webshop.* List of relations ┌─────────┬──────────────────────────┬──────┬──────────┬─────────┬─────────────┐ │ Schema │ Name │ Type │ Owner │ Size │ Description │ ├─────────┼──────────────────────────┼──────┼──────────┼─────────┼─────────────┤ │ webshop │ most_ordered_products │ view │ postgres │ 0 bytes │ │ │ webshop │ numbers_sold_per_article │ view │ postgres │ 0 bytes │ │ │ webshop │ numbers_sold_per_product │ view │ postgres │ 0 bytes │ │ └─────────┴──────────────────────────┴──────┴──────────┴─────────┴─────────────┘ (3 rows) Detailed Information Next, you can get all information on a single table with \\d+ \u0026lt;schema\u0026gt;.\u0026lt;tablename\u0026gt;:\njannikarndt@[local]:mywebshop=# \\d webshop.order Table \u0026#34;webshop.order\u0026#34; ┌───────────────────┬────────────────┬──────┬──────────┬─────────────────┐ │ Column │ Type │ Coll │ Nullable │ Default │ ├───────────────────┼────────────────┼──────┼──────────┼─────────────────┤ │ id │ integer │ │ not null │ nextval(ord... │ │ customerid │ integer │ │ │ │ │ ordertimestamp │ timestamp w... │ │ │ now() │ │ shippingaddressid │ integer │ │ │ │ │ total │ money │ │ │ │ │ shippingcost │ money │ │ │ │ │ created │ timestamp w... │ │ │ now() │ │ updated │ timestamp w... │ │ │ │ └───────────────────┴────────────────┴──────┴──────────┴─────────────────┘ Indexes: \u0026#34;order_pkey\u0026#34; PRIMARY KEY, btree (id) Foreign-key constraints: \u0026#34;fk_order_to_customer\u0026#34; FOREIGN KEY (customerid) REFERENCES customer(id) \u0026#34;order_shippingaddressid_fkey\u0026#34; FOREIGN KEY (shippingaddressid) REFERENCES address(id) Referenced by: TABLE \u0026#34;order_positions\u0026#34; CONSTRAINT \u0026#34;order_positions_orderid_fkey\u0026#34; FOREIGN KEY (orderid) REFERENCES \u0026#34;order\u0026#34;(id) Or for views:\njannikarndt@[local]:mywebshop=# \\d+ webshop.most_ordered_products View \u0026#34;webshop.most_ordered_products\u0026#34; ┌──────────────────────┬─────────┬───────────┬──────────┬─────────┬──────────┬──────┐ │ Column │ Type │ Collation │ Nullable │ Default │ Storage │ Desc │ ├──────────────────────┼─────────┼───────────┼──────────┼─────────┼──────────┼──────┤ │ id │ integer │ │ │ │ plain │ │ │ label │ text │ │ │ │ extended │ │ │ product │ text │ │ │ │ extended │ │ │ number_products_sold │ numeric │ │ │ │ main │ │ └──────────────────────┴─────────┴───────────┴──────────┴─────────┴──────────┴──────┘ View definition: SELECT products.id, labels.name AS label, products.name AS product, numbers_sold_per_product.number_products_sold FROM products products JOIN labels ON products.labelid = labels.id JOIN numbers_sold_per_product ON products.id = numbers_sold_per_product.productid ORDER BY numbers_sold_per_product.number_products_sold DESC LIMIT 20; Note that appending the + gives you the CREATE-statement for the view.\nUsers / Roles Finally, we should also have a look at the users:\njannikarndt@[local]:jannikarndt=# \\du+ List of roles ┌──────────────┬────────────────────────────────────┬────────────────┬─────────────┐ │ Role name │ Attributes │ Member of │ Description │ ├──────────────┼────────────────────────────────────┼────────────────┼─────────────┤ │ analytics │ │ {readonly} │ │ │ jannikarndt │ Superuser, Create role, Create DB │ {} │ │ │ mobile_app │ │ {write_access} │ │ │ postgres │ Superuser, Create role, Create DB, │ │ │ │ │ Replication, Bypass RLS │ │ │ │ readonly │ Cannot login │ {} │ │ │ web_app │ │ {write_access} │ │ │ write_access │ Cannot login │ {} │ │ └──────────────┴────────────────────────────────────┴────────────────┴─────────────┘ Officially, PostgreSQL doesn\u0026rsquo;t have users, only roles. The command however still uses the u.\nYou can see here, that there are two roles that cannot login (readonly and write_access). These are typically used to defined permissions. The roles that can login (analytics, web_app and mobile_app) are member of one of these roles so the admin doesn\u0026rsquo;t have to care about setting permissions for each of them.\nRecap To sum it up, these are the most important commands:\n\\l+ -- list databases \\c -- connect to a database \\dn+ -- list schemas \\dt+ \u0026lt;schema\u0026gt;.* -- list tables \\dv+ \u0026lt;schema\u0026gt;.* -- list views \\d+ \u0026lt;schema\u0026gt;.\u0026lt;relname\u0026gt; -- describe table or view \\du+ -- list users / roles ","permalink":"https://www.jannikarndt.de/blog/2018/08/postgresql_with_psql_pgcli/","summary":"\u003cp\u003eThe PostgreSQL installation comes with a great tool, \u003ccode\u003epsql\u003c/code\u003e, to administer and inspect the database. \u003ca href=\"https://www.pgcli.com\"\u003e\u003ccode\u003epgcli\u003c/code\u003e\u003c/a\u003e extends this with syntax highlighting and autocompletion.\u003c/p\u003e","title":"Examining a PostgreSQL with psql or pgcli"},{"content":"I want my photos to have location info in them, and Nikon wants way too much money for that. So I\u0026rsquo;ll do this: Let my Apple Watch track where I go, using the Outdoor Walk, export the route as GPX and use exiftool to tag all my images. Here\u0026rsquo;s how I do that.\nExport GPX Apples apps don\u0026rsquo;t come with an export option, so I use the wonderful little app called RunGap. An in-app-purchase for 2.29€ allows my to export a GPX file of my workout:\n   There\u0026rsquo;s also a great website that let\u0026rsquo;s you analyse the contents of that file:\n\nWrite GPS into EXIF Next, I use the software to edit EXIF data, written in Perl, first published in 2003, still state of the art today: exiftool. You can install it via\nbrew install exiftool (on a mac, of course).\nThen the command is pretty straightforward:\n$ ~ exiftool -geotag=myRoute.gpx ~/Pictures/UntaggedPictures/ 1 directories scanned 83 image files updated …and it\u0026rsquo;s done!\nThe exiftool can do way more, by the way, they have a page dedicated just for geotagging. But for starters, this does the job pretty well!\n","permalink":"https://www.jannikarndt.de/blog/2018/08/using_your_apple_watch_workout_to_geotag_dslr_photos/","summary":"\u003cp\u003eI want my photos to have location info in them, and Nikon wants \u003cem\u003eway too much\u003c/em\u003e money for that. So I\u0026rsquo;ll do this: Let my Apple Watch track where I go, using the \u003cem\u003eOutdoor Walk\u003c/em\u003e, export the route as GPX and use \u003cem\u003eexiftool\u003c/em\u003e to tag all my images. Here\u0026rsquo;s how I do that.\u003c/p\u003e","title":"Using your Apple Watch Workout to Geotag DSLR Photos"},{"content":"You don\u0026rsquo;t need to “get hacked” to have your security compromised. Often enough you\u0026rsquo;ll do it yourself. The best way to prevent this is knowing when to be cautious.\n1. Checked into git Checking credentials into git is such a regular thing that toughworks have created their own tool to prevent this.\nWhat you can do about it If it\u0026rsquo;s already pushed, you\u0026rsquo;ll have to rotate the crendentials.\nTo at least soften the embarrasment, you can use git commit --amend, and then git push -f, but that might\n get you into even bigger trouble, because your team will hate you, not work, for example if you pushed to a merge request in Gitlab it will keep the old code, despite what\u0026rsquo;s stored in git lead you (or others) to think that you don\u0026rsquo;t have to rotate the keys. You do.  How to prevent this  Don\u0026rsquo;t add passwords to the code, read them from the environment. Use direnv to have different environments for each project. Add the .envrc to your ~/.gitignore_global (for yourself) and the project\u0026rsquo;s .gitignore (for the others).  You might also consider talisman, but for me it had way too many false positives.\nAlso, if you want to store credentials inside git, try out git-crypt. But beware: You have to first add a filename to .gitattributes and then add the file to git.\n2. Dumped from the Environment When your application crashes, it often helps to know the environment. That\u0026rsquo;s why tools like sentry will collect and display all environments variables with any error.\nBut you might also write them to the standard out yourself, to have Elasticsearch pick them up.\nWhat you can do about it If you notice credentials showing up in another tool or a place where they don\u0026rsquo;t belong: Rotate. This is especially true for tools that tend to be used by a lot of people in your organization, which is usually the case for an Elastic stack.\nHow to prevent this It depends on the tools you use. Be aware of the possibility, try what happens when errors are reported and work around it.\n3. Written to the Chat “I can\u0026rsquo;t get this to run, can you send me the command?”\nUsing passwords in command line arguments should be a no-go, but liquibase for example enforces this behaviour.\nWhat you can do about it Would you really consider your chat as secure? Rather rotate.\nHow to prevent this In the liquibase example, you can write a wrapper-bash-script that reads the password from the environment, like this one:\n#!/bin/sh  RED=\u0026#39;\\033[0;31m\u0026#39; BLUE=\u0026#39;\\033[0;34m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; # No Color ENV=$1 COMMAND=${@:2} function usage { echo \u0026#34;Usage: ./lb \u0026lt;env\u0026gt; \u0026lt;command\u0026gt;\u0026#34; echo \u0026#34; \u0026lt;env\u0026gt; = local, dev, prod\u0026#34; echo \u0026#34; \u0026lt;command\u0026gt; = update, status, rollbackCount 1, ...\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;Configure your .envrc for database access:\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;${BLUE}# local\u0026#34; echo \u0026#34;export HOST_PORT_DB_LOCAL=localhost:5431/postgres\u0026#34; echo \u0026#34;export DB_USER_LOCAL=postgres\u0026#34; echo \u0026#34;export DB_PASSWORD_LOCAL=...\\n\u0026#34; echo \u0026#34;# dev\u0026#34; echo \u0026#34;export HOST_PORT_DB_DEV=localhost:5434/postgres\u0026#34; echo \u0026#34;export DB_USER_DEV=postgres\u0026#34; echo \u0026#34;export DB_PASSWORD_DEV=...\\n\u0026#34; echo \u0026#34;# prod\u0026#34; echo \u0026#34;export HOST_PORT_DB_PROD=localhost:5433/postgres\u0026#34; echo \u0026#34;export DB_USER_PROD=postgres\u0026#34; echo \u0026#34;export DB_PASSWORD_PROD=...${NC}\u0026#34; } function setDbConnection { case $ENV in \u0026#34;local\u0026#34;) HOST_PORT_DB=${HOST_PORT_DB_LOCAL} DB_USER=${DB_USER_LOCAL} DB_PASSWORD=${DB_PASSWORD_LOCAL} ;; \u0026#34;dev\u0026#34;) HOST_PORT_DB=${HOST_PORT_DB_DEV} DB_USER=${DB_USER_DEV} DB_PASSWORD=${DB_PASSWORD_DEV} ;; \u0026#34;prod\u0026#34;) HOST_PORT_DB=${HOST_PORT_DB_PROD} DB_USER=${DB_USER_PROD} DB_PASSWORD=${DB_PASSWORD_PROD} ;; *) echo \u0026#34;${RED}Error: Missing environment!\\n${NC}\u0026#34; usage exit 1 esac if [[ -z \u0026#34;${HOST_PORT_DB}\u0026#34; || -z \u0026#34;${DB_USER}\u0026#34; || -z \u0026#34;${DB_PASSWORD}\u0026#34; ]]; then echo \u0026#34;${RED}Error: \\$HOST_PORT_DB, \\$DB_USER or \\$DB_PASSWORD is not defined!\\n${NC}\u0026#34; usage exit 1 fi } function checkForPostgresDriver { if [ ! -f ./postgresql.jar ]; then echo \u0026#34;${BLUE}Missing postgresql.jar, downloading…${NC}\\n\u0026#34; curl -o postgresql.jar https://jdbc.postgresql.org/download/postgresql-42.2.3.jar fi } function runLiquibase { liquibase \\  --driver=org.postgresql.Driver \\  --classpath=./postgresql.jar \\  --changeLogFile=changelog.xml \\  --contexts=$ENV \\  --url=\u0026#34;jdbc:postgresql://$HOST_PORT_DB\u0026#34; \\  --username=$DB_USER \\  --password=$DB_PASSWORD \\  ${COMMAND} } setDbConnection checkForPostgresDriver echo \u0026#34;${BLUE}Running liquibase on ${HOST_PORT_DB}with context ${NC}$ENV${BLUE}and command(s) ${NC}${COMMAND}${BLUE}…\\n${NC}\u0026#34; runLiquibase ","permalink":"https://www.jannikarndt.de/blog/2018/07/how_passwords_get_leaked/","summary":"\u003cp\u003eYou don\u0026rsquo;t need to “get hacked” to have your security compromised. Often enough you\u0026rsquo;ll do it yourself. The best way to prevent this is knowing when to be cautious.\u003c/p\u003e","title":"3 Ways How Passwords Get Leaked"},{"content":"\n\n\n","permalink":"https://www.jannikarndt.de/blog/2018/05/kirschbluetenfeuerwerk/","summary":"\u003cp\u003e\u003ca href=\"/blog/2018/05/Kirschbluetenfeuerwerk1.jpg\"\u003e\u003cimg src=\"/blog/2018/05/Kirschbluetenfeuerwerk1.jpg\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"Kirschblütenfeuerwerk"},{"content":"\n\n\n","permalink":"https://www.jannikarndt.de/blog/2018/05/russische_kirche_fernsehturm_landungsbruecken/","summary":"\u003cp\u003e\u003ca href=\"/blog/2018/05/Russische Kirche.jpg\"\u003e\u003cimg src=\"/blog/2018/05/Russische Kirche.jpg\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"Russische Kirche, Fernsehturm, Landungsbrücken"},{"content":"\n\n\n\n\n\n","permalink":"https://www.jannikarndt.de/blog/2018/04/boberg/","summary":"\u003cp\u003e\u003ca href=\"/blog/2018/04/Boberg1.jpg\"\u003e\u003cimg src=\"/blog/2018/04/Boberg1.jpg\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"Boberg"},{"content":"\n","permalink":"https://www.jannikarndt.de/blog/2018/04/alsterpanorama/","summary":"\u003cp\u003e\u003ca href=\"/blog/2018/04/Alsterpanorama.jpg\"\u003e\u003cimg src=\"/blog/2018/04/Alsterpanorama.jpg\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"Alsterpanorama"},{"content":"\n\n","permalink":"https://www.jannikarndt.de/blog/2018/04/hochwasserbassin/","summary":"\u003cp\u003e\u003ca href=\"/blog/2018/04/Hochwasserbassin1.jpg\"\u003e\u003cimg src=\"/blog/2018/04/Hochwasserbassin1.jpg\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2018/04/Hochwasserbassin2.jpg\"\u003e\u003cimg src=\"/blog/2018/04/Hochwasserbassin2.jpg\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"Hochwasserbassin"},{"content":"Adding an ssh-file as a secret sounds easy, but there are pitfalls.\nStep 1: Add secret to kubernetes First, add the key as a secret, for example with terraform\nresource \u0026#34;kubernetes_secret\u0026#34; \u0026#34;ssh_key_verstehensystem_csv_ingest_bwh\u0026#34; { metadata { name = \u0026#34;my-ssh-key\u0026#34; } data { \u0026#34;id_rsa\u0026#34; = \u0026#34;${file(\u0026#34;id_rsa\u0026#34;)}\u0026#34; } type = \u0026#34;Opaque\u0026#34; } (see Docs) or with kubectl:\n$ kubectl create secret generic my-ssh-key --from-file=id_rsa=/path/to/local-ssh-keys (see Docs). Note that this command renames the file: --from-file=\u0026lt;name on the cluster\u0026gt;=\u0026lt;local file\u0026gt;.\nStep 2: Mount the secret Now, in your pod, mount the secret as a volume:\nkind:PodapiVersion:v1metadata:name:...spec:containers:- name:...image:...volumeMounts:- name:ssh-key-volumemountPath:\u0026#34;/etc/ssh-key\u0026#34;volumes:- name:ssh-key-volumesecret:secretName:my-ssh-keydefaultMode:256Step 3: Do\u0026rsquo;s and Dont\u0026rsquo;s Mount to ~/.ssh/ Remember that mounting to an existing directory will overwrite it. Even if .ssh does not exist, it will be replaced by a read-only mount, so ssh will fail when it creates the known_hosts file.\nBesides, writing ~ in your yaml will most likely create a folder called '~'. You need absolute paths!\nForget the defaultMode ssh checks the key\u0026rsquo;s file permissions and will fail if they are too broad. Since the volume is read-only, you cannot simply chmod after the fact, you need to set the permissions in your yaml. But beware…\nWrite POSIX in defaultMode The docs state:\n Note that the JSON spec doesn’t support octal notation, so use the value 256 for 0400 permissions. If you use yaml instead of json for the pod, you can use octal notation to specify permissions in a more natural way.\n In my experience, the yaml spec also does not support octal notation, so you need to convert:\n400 = (4 * 8^2) + (4 * 8^1) + (4 * 8^0) = (4 * 64) + (0 * 8) + (0 * 1) = 256 + 0 + 0 = 256 ","permalink":"https://www.jannikarndt.de/blog/2018/03/ssh_key_as_kubernetes_secret/","summary":"\u003cp\u003eAdding an ssh-file as a secret sounds easy, but there are pitfalls.\u003c/p\u003e","title":"How to add an SSH Key as a Kubernetes Secret"},{"content":"I reduced the startup time of my shell by one second. Here\u0026rsquo;s how:\nWhat I do I work a lot with the shell (or “Terminal” app on MacOS), mostly kubectl, git, terraform and docker. And of course I use the absolutely best shell of all, oh-my-zsh.\nThe Problem I noticed that recently the startup time for a new shell (or new tab) has grown longer than a second. That\u0026rsquo;s annoying, because usually when I open a new tab, I want to quickly check on something, like the logs of a pod or if I have committed something in a different project.\nI inserted some gdate +%s.%N commands in my .zshrc to find out, at which point the slowdown was occurring. (gdate is the gnu-version of date and can be installed with brew install coreutils.)\nThe problem lies in the kubectl plugin, which creates the completions-list via source \u0026lt;(kubectl completion zsh). This creates a huge list of instructions.\nThe Solution  The kubectl plugin also defines the alias k=kubectl, which shaves seconds, if not minutes off my daily work. When I start a new shell, I expect it to be fast. When I check on something in the Kubernetes cluster I expect it to go through the network, i.e. to be slow.  So I delayed the initialization of the kubectl completion until I first call something:\nfunction k() { source \u0026lt;(kubectl completion zsh) alias k=kubectl kubectl $@ } This way, when I write k get pods for the first time, it creates the autocompletion list, rebinds what k does and then runs the command I wanted in the first place. All following k get pods go directly to kubectl.\n","permalink":"https://www.jannikarndt.de/blog/2018/03/lazy_aliases_in_my_shell/","summary":"I reduced the startup time of my shell by one second. Here\u0026rsquo;s how:\nWhat I do I work a lot with the shell (or “Terminal” app on MacOS), mostly kubectl, git, terraform and docker. And of course I use the absolutely best shell of all, oh-my-zsh.\nThe Problem I noticed that recently the startup time for a new shell (or new tab) has grown longer than a second. That\u0026rsquo;s annoying, because usually when I open a new tab, I want to quickly check on something, like the logs of a pod or if I have committed something in a different project.","title":"Lazy Aliases in my Shell"},{"content":"Apples Time Machine is a great backup solution, you only have to do one thing: Connect your disk from time to time.\nSince that is way harder than it sounds, there\u0026rsquo;s a second option: Buy a 329€ Time Capsule and do backups over wifi!\nThat\u0026rsquo;s too expensive? Here\u0026rsquo;s how to build your own Time Capsule with a Raspberry Pi 3 and an external hard drive.\nThis guide is based on https://www.bersling.com/2017/01/02/time-capsule-time-machine-through-raspberry-pi/, but tries to be more comprehensive and also includes a few workarounds for problems I encountered.\nOn the pi 1. Connect USB drive to pi 2. Find drive $ lsblk -o KNAME,TYPE,SIZE,MODEL KNAME TYPE SIZE MODEL sda disk 465.8G MK5065GSXF \u0026lt;= probably that one mmcblk0 disk 14.9G mmcblk0p1 part 41.5M mmcblk0p2 part 14.9G 3. Format drive $ sudo mkfs.ext4 /dev/sda 4. Install netatalk $ sudo apt-get update $ sudo apt-get upgrade $ sudo apt-get install netatalk 5. Create a mount point $ sudo mkdir /mnt/TimeMachine 6. Add mount to file system table (fstab) $ sudo nano /etc/fstab Insert the mount post at the end of the file: /dev/sda /mnt/TimeMachine auto defaults 0 2 and exit with ctrl + x, y, enter.\n7. Mount device $ sudo mount /dev/sda 8. Make it accessible $ sudo chmod 777 /mnt/TimeMachine 9. Make the drive known to netatalk $ sudo nano /etc/netatalk/AppleVolumes.default Add /mnt/TimeMachine \u0026quot;Time Machine\u0026quot; options:tm at the end, then exit with ctrl + x, y, enter\n10. Restart netatalk $ sudo service netatalk restart 11. Restart the pi $ sudo reboot On the Mac 1. Allow unsupported devices In Terminal, make Time Machine show unsupported devices with\n$ defaults write com.apple.systempreferences TMShowUnsupportedNetworkVolumes 1 2. Find the remote disk Open the Finder and click on raspberrypi in the network interfaces\nConnect as pi and your password. Then double click the Time Machine disk:\n3. Add the disk to Time Machine Head to the Time Machine Preferences and add the new disk:\n","permalink":"https://www.jannikarndt.de/blog/2018/01/how_to_use_a_raspberry_pi_for_your_time_machine_backups/","summary":"\u003cp\u003eApples Time Machine is a great backup solution, you only have to do \u003cem\u003eone\u003c/em\u003e thing: Connect your disk from time to time.\u003c/p\u003e\n\u003cp\u003eSince that is way harder than it sounds, there\u0026rsquo;s a second option: Buy a \u003ca href=\"https://www.apple.com/de/shop/product/ME177Z/A\"\u003e329€ Time Capsule\u003c/a\u003e and do backups over wifi!\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s too expensive? Here\u0026rsquo;s how to build your own Time Capsule with a Raspberry Pi 3 and an external hard drive.\u003c/p\u003e","title":"How to use a Raspberry Pi for your Time Machine backups"},{"content":"Sonoff takes the standard ESP8266 chip and adds two things:\n a casing with wifi and great connectors and a custom OS and an app to control the devices.  Unfortunately, the second addition also means that all communication goes through Sonoff\u0026rsquo;s servers. Here\u0026rsquo;s how to get rid of that. Without soldering or connecting anything.\nThe two repositories I\u0026rsquo;m using are\n https://github.com/arendst/Sonoff-Tasmota for the Tasmota operating system and https://github.com/mirko/SonOTA for a mechanism to flash the device “over the air”.  1. Get SonOTA Code First you need to clone (=download) the repository and install the requirements. For this you need git and pip.\n$ git clone https://github.com/mirko/SonOTA $ cd SonOTA $ pip3 install --user -r requirements.txt 2. Allow incoming and outgoing connections Your mac will serve as a server. Therefore it must allow incoming connections. Go to System Settings \u0026gt; Security \u0026amp; Privacy \u0026gt; Firewall and turn the firewall off. Remember to turn it back on afterwards!\nI also noticed LittleSnitch and TripMode trying to block connections. If you have them installed, put them into silent mode or turn them off.\n3. Start SonOTA script $ ./sonota.py Current IPs: [\u0026#39;192.168.31.210\u0026#39;] Select IP address of the WiFi interface: 0: 192.168.31.210 Select IP address [0]: 0 # \u0026lt;= enter \u0026#39;0\u0026#39; and enter WiFi SSID: Your SSID # \u0026lt;= no quotes WiFi Password: ***** 4. Put Sonoff into config mode Press the button on the Sonoff device for ~7 seconds until the LED starts blinking rapidly.\nIf the LED is blinking three times short and then one time long, you need to hold the button for 7 seconds again, until it blinks rapidly. I had to do this for the Sonoff Touch.\n5. Join Sonoff Wifi In config mode, the Sonoff broadcasts its own wifi.\n Look for and connect to ITEAD-100001XXXX The password is 12345678  6. Let the script configure the sonoff \u0026gt;\u0026gt; HTTP GET /10.10.7.1/device # ... \u0026gt;\u0026gt; HTTP POST /10.10.7.1/ap # ... ~~ Provisioning completed Starting stage2... ** The IP address of \u0026lt;serve_host\u0026gt; (192.168.31.210) is not assigned to any interface on this machine. ** Please change WiFi network to Tor zur Welt and make sure 192.168.31.210 is being assigned to your WiFi interface. ** This application should be kept running and will wait until connected to the WiFi... ......Current IPs: [] The device now joins your wifi and ends providing its own (ITEAD-100001XXXX). Your computer will most likely switch back to your usual wifi.\n....Current IPs: [\u0026#39;192.168.31.210\u0026#39;] ~~ Starting web server (HTTP port: 8080, HTTPS port 8443) ~~ Waiting for device to connect # ... \u0026lt;\u0026lt; HTTP POST /dispatch/device 200 POST /dispatch/device (192.168.31.213) 101 GET /api/ws (192.168.31.213) \u0026lt;\u0026lt; WEBSOCKET OPEN # ... (lots of \u0026#34;action\u0026#34;: \u0026#34;update\u0026#34; payloads) # ... Sending file: /ota/image_user2-0x81000.bin # repeats a lot # ... ~~~ device acknowledged our action request (seq 1515773926583) with error code 0 ........................ *** IMPORTANT! *** ** AFTER the first download is COMPLETE, with in a minute or so you should connect to the new SSID \u0026#34;FinalStage\u0026#34; to finish the process. ** ONLY disconnect when the new \u0026#34;FinalStage\u0026#34; SSID is visible as an available WiFi network. This server should automatically be allocated the IP address: 192.168.4.2. If you have successfully connected to \u0026#34;FinalStage\u0026#34; and this is not the IP Address you were allocated, please ensure no other device has connected, and reboot your Sonoff. 7. Switch to FinalStage Wifi If you have successfully connected to \u0026#34;FinalStage\u0026#34; and this is not the IP Address you were allocated, please ensure no other device has connected, and reboot your Sonoff. ..Current IPs: [] .Sending file: /ota/image_arduino.bin .Current IPs: [\u0026#39;192.168.4.2\u0026#39;] The \u0026#34;FinalStage\u0026#34; SSID will disappear when the device has been fully flashed and image_arduino.bin has been installed. If there is no \u0026#34;Sending file: /ota/image_arduino.bin\u0026#34; log entry, ensure all firewalls have been COMPLETELY disabled on your system. ............200 GET /ota/image_arduino.bin (192.168.4.1) 25854.05ms ..............Current IPs: [] No longer on \u0026#34;FinalStage\u0026#34; SSID, all done! Now connect to the sonoff-#### SSID and configure for your WiFi (it will not be configured). Quitting. 8. Switch to sonoff-XXXX Wifi  Joining this Wifi might take 3–4 minutes Remember the numbers! Goto http://192.168.4.1  Enter your Wifi SSID and password Save The sonoff-XXXX Wifi will disappear and your computer will return to your usual wifi  9. Configure Device  Goto http://sonoff-XXXX.local where the number is the one from step 8  Go to Configuration \u0026gt; Configure Module and select the device type   10. Configure MQTT The idea here is that you have an MQTT broker somewhere to exchange messages between your smart devices and an app to control them. I use homebridge on a Raspberry Pi, here is a tutorial for my setup.\nIf you want to try without a raspberry pi, you can run the broker on your computer:\nbrew install mosquitto mosquitto Username and Password defaults are empty.\nIf brew has problems linking mosquitto, try\nsudo mkdir /usr/local/sbin sudo chown -R `whoami`:admin /usr/local/sbin brew link mosquitto Now follow these steps to let your device send messages to your MQTT broker:\n Go to Configuration \u0026gt; Configure MQTT and enter the MQTT parameters:  Host: e.g. your ip if MQTT is running locally Port: 1883 Topic: Specific Topic, e.g. bedroom_lights    Restart Go to Console to check the connection   You can use a program like MQTT.fx and subscribe to everything (“#”) to see how your devices communicate:\n11. Create a smart home Now that your lights, outlets, fans and what not talk to a message queue and receiver orders, you need something to control that. For example, setup a homebridge on a Raspberry Pi to use Apple Home from your iPhone, iPad or Apple Watch! Here\u0026rsquo;s how.\n","permalink":"https://www.jannikarndt.de/blog/2018/01/how_to_install_tasmota_on_a_sonoff_device_without_opening_it/","summary":"\u003cp\u003eSonoff takes the standard ESP8266 chip and adds two things:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea casing with wifi and great connectors and\u003c/li\u003e\n\u003cli\u003ea custom OS and an app to control the devices.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUnfortunately, the second addition \u003cem\u003ealso\u003c/em\u003e means that all communication goes through Sonoff\u0026rsquo;s servers. Here\u0026rsquo;s how to get rid of that. Without soldering or connecting anything.\u003c/p\u003e","title":"How to install Tasmota on a Sonoff device without opening it"},{"content":"“Hey Siri, turn on the bedroom lights!” I want that. Here\u0026rsquo;s how I did it:\n I bought a bunch of Sonoff devices (5€ each, 10€ for a light switch). I bought a raspberry pi (33€). I installed an MQTT broker and homebridge on the pi.  1. Set up the pi =\u0026gt; See How to setup a Raspberry Pi 3 headless, without monitor, mouse or keyboard\n2. Install mosquitto and homebridge …and everything else you need:\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade $ sudo apt-get install mosquitto $ sudo apt-get install make $ sudo apt-get install nodejs $ sudo apt-get install libavahi-compat-libdnssd-dev $ sudo apt-get install npm $ sudo npm install -g homebridge $ sudo npm install -g homebridge-mqtt-switch-tasmota # for sonoff devices 3. Configure mosquitto The default for mosquitto is to run without any security. Let\u0026rsquo;s not do this. This creates a user “home” with a password:\n$ sudo mosquitto_passwd -c /etc/mosquitto/passwd home Password: yourpassword Reenter password: yourpassword This will create a password file. You can look at it with\n$ cat /etc/mosquitto/passwd home:$6$yjSnOc95804YRW/E$lokE/zzg4XwKj1BJPOxXDq4njkeovnecAvtYCOmNYgn5v/c8sHP08LnH7rDP0uU59hzmV/5iTXsudDrO6RMWPl+A== Now we need to tell mosquitto to use this password file:\n$ sudo nano /etc/mosquitto/mosquitto.conf Add the lines\npassword_file /etc/mosquitto/passwd allow_anonymous false Exit with ctrl + x, y and enter.\nNow restart the daemon:\n$ sudo systemctl restart mosquitto You can check the status with\n$ sudo /etc/init.d/mosquitto status 4. Configure homebridge First we\u0026rsquo;ll create a config file for homebridge and open it:\n$ mkdir ~/.homebridge $ touch ~/.homebridge/config.json $ nano ~/.homebridge/config.json Your config should look something like this:\n{ \u0026#34;bridge\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Raspberry Pi\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;XX:XX:XX:XX:XX:XX\u0026#34;, \u0026#34;port\u0026#34;: 51826, \u0026#34;pin\u0026#34;: \u0026#34;XXX-XX-XXX\u0026#34; }, \u0026#34;platforms\u0026#34;: [], \u0026#34;accessories\u0026#34;: [{ \u0026#34;accessory\u0026#34;: \u0026#34;mqtt-switch-tasmota\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;My Smart Device\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;mqtt://127.0.0.1\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;your password from step 3\u0026#34;, \u0026#34;topics\u0026#34;: { \u0026#34;statusGet\u0026#34;: \u0026#34;stat/topic_for_this_device/POWER\u0026#34;, \u0026#34;statusSet\u0026#34;: \u0026#34;cmnd/topic_for_this_device/POWER\u0026#34;, \u0026#34;stateGet\u0026#34;: \u0026#34;tele/topic_for_this_device/STATE\u0026#34; } }, {} ] } For more information have a look at the config.json sample or the page of the plugin you\u0026rsquo;re using (mqtt-switch-tasmota in my case).\nIf you don\u0026rsquo;t like nano you can also copy the file to your computer, edit it there and then copy it back:\n$ scp pi@192.168.31.231:.homebridge/config.json ~/Downloads/homebridge.json config.json 100% 1830 149.2KB/s 00:00 # edit $ scp ~/Downloads/homebridge.json pi@192.168.31.231:.homebridge/config.json homebridge.json 100% 1457 268.4KB/s 00:00 But beware: Do not edit the file in TextEdit, as it changes the format.\nYou should now be able to start the homebridge app:\n$ homebridge 5. Start homebridge on startup Great! Now all that\u0026rsquo;s left is to create a user and a service to run homebridge on startup. For this I followed this guide:\n Create a file for default parameters  $ sudo nano /etc/default/homebridge and paste HOMEBRIDGE_OPTS=-U /var/homebridge into the file. Quit with ctrl + x, y, enter.\n Create a service in systemd  $ sudo nano /etc/systemd/system/homebridge.service and paste\n[Unit] Description=Node.js HomeKit Server After=syslog.target network-online.target # [Service] Type=simple User=homebridge EnvironmentFile=/etc/default/homebridge ExecStart=/usr/local/bin/homebridge $HOMEBRIDGE_OPTS Restart=on-failure RestartSec=10 KillMode=process # [Install] WantedBy=multi-user.target  Create a user homebridge  $ sudo useradd --system homebridge  Create a directory for the config  $ sudo mkdir /var/homebridge $ sudo cp ~/.homebridge/config.json /var/homebridge/ $ sudo cp -r ~/.homebridge/persist /var/homebridge  Start the service  $ sudo chmod -R 0777 /var/homebridge $ sudo systemctl daemon-reload $ sudo systemctl enable homebridge $ sudo systemctl start homebridge $ systemctl status homebridge Notice that the config.json is now copied to a different folder, so if you change the one in ~/.homebridge/ you need to copy it to /var/homebridge/ afterwards!\n$ sudo cp ~/.homebridge/config.json /var/homebridge/ $ sudo systemctl restart homebridge 6. Add smart devices You now have control center for you smart devices! Here is a guide on how to connect them!\n","permalink":"https://www.jannikarndt.de/blog/2018/01/how_to_use_a_raspberry_pi_3_with_apple_home/","summary":"\u003cp\u003e“Hey Siri, turn on the bedroom lights!” I want that. Here\u0026rsquo;s how I did it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI bought a bunch of Sonoff devices (5€ each, 10€ for a light switch).\u003c/li\u003e\n\u003cli\u003eI bought a raspberry pi (33€).\u003c/li\u003e\n\u003cli\u003eI installed an MQTT broker and homebridge on the pi.\u003c/li\u003e\n\u003c/ul\u003e","title":"How to use a Raspberry Pi 3 with Apple Home"},{"content":"I bought a raspberry pi as a smart home automation server. Here\u0026rsquo;s how to set it up without connecting a monitor, mouse or keyboard. All you need is an ethernet cable.\n1. Prepare the SD card 1. Download Raspbian Here, -L means follow redirect, -C - let\u0026rsquo;s you resume the download.\n$ curl https://downloads.raspberrypi.org/raspbian_lite_latest -L -C - -o raspbian-stretch-lite.zip % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 390 100 390 0 0 69 0 0:00:05 0:00:05 --:--:-- 89 100 399 100 399 0 0 70 0 0:00:05 0:00:05 --:--:-- 70 20 351M 20 72.6M 0 0 358k 0 0:16:44 0:03:27 0:13:17 364k 2. Unzip the Download $ tar xzf raspbian-stretch-lite.zip 3. Check the Path of the SD Card Here it is /dev/disk2:\n$ diskutil list /dev/disk0 (internal): ... /dev/disk1 (synthesized): ... /dev/disk2 (external, physical): \u0026lt;= that\u0026#39;s the one ... /dev/disk3 (disk image): ... 4. Unmount the SD Card $ diskutil unmountDisk /dev/disk2 5. Copy the Data to the Card We use /dev/rdisk2 because it\u0026rsquo;s a lot faster then /dev/disk2:\n$ sudo dd bs=1m if=2018-11-13-raspbian-stretch-lite.img of=/dev/rdisk2 conv=sync Password: ******** 1780+0 records in 1780+0 records out 1866465280 bytes transferred in 33.915228 secs (55033252 bytes/sec) 6. Enable one-time SSH Access $ touch /Volumes/boot/ssh 7. Eject the SD Card $ diskutil eject /dev/disk2 2. Get Network Access to the Pi Connected to your Router You now have two options to connect to the pi via network: either you plug it into your router and look at the DHCP settings what IP address is assigned to it: Connected to your Mac You can also connect it to your Mac directly. For that, you also need to enable Internet Sharing in System Settings \u0026gt; Sharing: 3. Configure SSH access This assumes that you have an ssh key. If not or you don\u0026rsquo;t know what that is: An ssh key consist of two files: A private and a public one. The private one (id_rsa) is on your computer and works like a password (so do not share it!). The public one (id_rsa.pub) is on other computers and identifies you. It only works together with your private key, so don\u0026rsquo;t loose it. Your public key is on your computer as well so you can easily share it.\nYou can find both keys with\n$ ls -l ~/.ssh total 120 -rw------- 1 jannikarndt staff 1766 Jan 3 2017 id_rsa -rw-r--r--@ 1 jannikarndt staff 403 Jan 3 2017 id_rsa.pub If you don\u0026rsquo;t have a key, GitHub has a great article on how to create one.\n1. Copy your SSH Key to the Pi $cat ~/.ssh/id_rsa.pub | ssh pi@192.168.2.149 \u0026#34;mkdir -p ~/.ssh \u0026amp;\u0026amp; cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026#34; The authenticity of host \u0026#39;192.168.2.149 (192.168.2.149)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:TDaxHjcZfoPqgvY2Mq0RVvcakKlEsU9AntEzicUXl6U. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;192.168.2.149\u0026#39; (ECDSA) to the list of known hosts. pi@192.168.2.149\u0026#39;s password: raspberry 2. SSH into the Pi $ ssh pi@192.168.2.2 Linux raspberrypi 4.14.79-v7+ #1159 SMP Sun Nov 4 17:50:20 GMT 2018 armv7l The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. SSH is enabled and the default password for the \u0026#39;pi\u0026#39; user has not been changed. This is a security risk - please login as the \u0026#39;pi\u0026#39; user and type \u0026#39;passwd\u0026#39; to set a new password. Wi-Fi is disabled because the country is not set. Use raspi-config to set the country before use. The preconfigured password is raspberry. A good reason to change it right away:\n3. Change your root Password: sudo raspi-config   Add your wifi credentials in 2 Network Options \u0026gt; N2 Wi-fi\n  Permanently enable ssh access in 5 Interfacing Options \u0026gt; P2 SSH.\n  4. Install oh-my-zsh And now, for the grand finale, you can (should / will want to) install a proper shell, i.e. oh my zsh:\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade sudo apt-get install git zsh chsh -s /bin/zsh sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026#34; 5. Continue Great! Now you can use your Pi, for example\n with Apple Home and Sonoff devices to control your smart home or as an alternative to Apples 329€ Time Capsule for Time Machine backups over wifi. ","permalink":"https://www.jannikarndt.de/blog/2018/01/raspberry_pi_3_headless_setup/","summary":"\u003cp\u003eI bought a raspberry pi as a smart home automation server. Here\u0026rsquo;s how to set it up without connecting a monitor, mouse or keyboard. All you need is an ethernet cable.\u003c/p\u003e","title":"How to setup a Raspberry Pi 3 headless, without monitor, mouse or keyboard"},{"content":"This is a quickstart for building something on Google Compute Engine without clicking any buttons (after you created the project).\nPrerequisites Install terraform:\n$ brew install terraform Install Google Cloud SDK:\n$ brew cask install google-cloud-sdk create a new project in the console and login\n$ gcloud auth application-default login If you are using IntelliJ IDEA, install the HashiCorp Terraform Plugin.\nOption A: Import a project If you have created a project using the console, create a config.tf with the basic settings:\nprovider \u0026#34;google\u0026#34; { region = \u0026#34;eu-central-1\u0026#34; } resource \u0026#34;google_project\u0026#34; \u0026#34;project\u0026#34; {} Now run terraform init to download the google provider plugin. Now import the project via\n$ terraform import google_project.project project-id-186346 You now have a corresponding terraform.tfstate file that contains the name, billing account and other info about your project.\nOption B: Create a new project from scratch Make a new folder and create a config.tf file:\nprovider \u0026#34;google\u0026#34; { region = \u0026#34;eu-central-1\u0026#34; } resource \u0026#34;google_project\u0026#34; \u0026#34;project\u0026#34; { name = \u0026#34;holisticon\u0026#34; project_id = \u0026#34;holisticon-123456\u0026#34; billing_account = \u0026#34;01B8C8-F33191-3DE337\u0026#34; // optional } Now run terraform init to download the google provider plugin. Next run terraform apply to create the project. Note that the project_id may not already exist. The billing_account is optional. Also you can only have a maximum of 12 projects at the same time.\nAdding people So far you are the owner of the new project. Now you can create IAM roles and add other people to your project:\nresource \u0026#34;google_project_iam_binding\u0026#34; \u0026#34;project_editors\u0026#34; { project = \u0026#34;${google_project.project.project_id}\u0026#34; role = \u0026#34;roles/editor\u0026#34; members = [ \u0026#34;user:nice.coworker@holisticon.de\u0026#34;, ] } You can find a description of all roles here. Note that you cannot grant the owner role through the API but only using the Cloud Platform Console.\n","permalink":"https://www.jannikarndt.de/blog/2017/11/terraform_on_google_cloud_engine_quickstart/","summary":"\u003cp\u003eThis is a quickstart for building something on Google Compute Engine without clicking any buttons (after you created the project).\u003c/p\u003e","title":"Terraform on Google Cloud Engine Quickstart"},{"content":"#1 Conway’s Law  \u0026ldquo;organizations which design systems \u0026hellip; are constrained to produce designs which are copies of the communication structures of these organizations.\u0026rdquo;\n So true.\n#2 Dev-Ops is awesome But also a lot of work that’s hard to anticipate.\n#3 Clearly defined interfaces clearly need to be tested We didn’t test ours, and it took other dev forever to track down a bug in our system.\n#4 Deal with the bugs you have, not the ones you might encounter Your software will never handle all possibilities. Invest the time into good monitoring, rather than anticipating every possibility.\n#5 How to set up a robust environment  Set up a dev, test and production stage. Set up continuous deployment into all stages. Set up a monitoring system. Start coding.  #6 Your system doesn’t need every bit of new technology And it probably doesn’t have “Big Data”.\n#7 MonolithFirst As Martin Fowler writes:\n you shouldn\u0026rsquo;t start a new project with microservices, even if you\u0026rsquo;re sure your application will be big enough to make it worthwhile\n","permalink":"https://www.jannikarndt.de/blog/2017/11/lessons_learned_in_2017/","summary":"\u003ch2 id=\"1\"\u003e#1\u003c/h2\u003e\n\u003ch3 id=\"conways-law\"\u003eConway’s Law\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;organizations which design systems \u0026hellip; are constrained to produce designs which are copies of the communication structures of these organizations.\u0026rdquo;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSo true.\u003c/p\u003e","title":"Lessons learned in 2017"},{"content":"I was quite surprised at how difficult it is to get absolutely razor sharp images out of a very good (high-end-consumer/semi-pro) DSLR camera, namely the Nikon D7100. So I made the test: What settings in terms of aperture, shutter speed and ISO yield the sharpest landscape image?\nThe setup I tried to keep the control variables constant:\n Camera (Nikon D7100), lens (Nikon AF Nikkor 50mm 1:1,8D) and scene, obviously Camera mounted on a tripod 2 second self timer, so pressing the shutter release doesn\u0026rsquo;t blur the image All images in RAW format, imported in Nikon\u0026rsquo;s ViewNX-i app, Sharpness turned to 10 (max) and exported als full resolution jpegs.  Aperture, Shutter Speed and ISO As you might know, the brightness of an image is influences by the aperture (how much light is going through the lens), the shutter speed (for how long is the light coming through the lens) and ISO (how sensitive is the film/chip).\nBut varying these also has other effects:\n A small aperture (i.e. wide open lens, for example ƒ/1.8) results in the depth of field effect you use for portraits. A closed lens (e.g. ƒ/22) makes sharper images, but lets through very little light. A long exposure, i.e. low shutter speed, for example 1 sec, lets through a lot of light but everything that moves is blurred — including the whole picture, if you move the camera. A short exposure/high shutter speed (e.g. 1/600 sec) “freezes” the image, but also lets through very little light A high ISO can help if you don\u0026rsquo;t get enough light otherwise, but it results in “noise”. This is an area where camera models still improve with every iteration.  The pictures I started with the lowest ISO (100) and widest aperture (ƒ/1.8), requiring a shutter speed of 1/80. From there on I decreased the aperture and shutter speed so the lighting would stay the same. After that I increased the ISO, trying to reach a shutter speed where I could spare the tripod (1/50 sec or faster). The last version is a reasonable mix of minimum aperture (ƒ/8), minimum shutter speed (1/100 sec) and maximum ISO (6400).\n   Aperture Shutter Speed ISO Image     ƒ/1.8 1/80 s 100    ƒ/4 1/20 s 100    f/8 1/4 s 100    f/14 0.8 s 100    f/22 2.0 s 100    f/22 1.3 s 250    f/22 1/2 s 640    f/22 1/5 s 1600    f/22 1/10 s 3200    f/22 1/25 s 8000    f/8 1/100 s 6400     Since the differences are very subtle, I chose three areas that I zoomed in on:\n   Aperture Shutter Speed ISO Image     ƒ/1.8 1/80 s 100    ƒ/4 1/20 s 100    f/8 1/4 s 100    f/14 0.8 s 100    f/22 2.0 s 100    f/22 1.3 s 250    f/22 1/2 s 640    f/22 1/5 s 1600    f/22 1/10 s 3200    f/22 1/25 s 8000    f/8 1/100 s 6400        Aperture Shutter Speed ISO Image     ƒ/1.8 1/80 s 100    ƒ/4 1/20 s 100    f/8 1/4 s 100    f/14 0.8 s 100    f/22 2.0 s 100    f/22 1.3 s 250    f/22 1/2 s 640    f/22 1/5 s 1600    f/22 1/10 s 3200    f/22 1/25 s 8000    f/8 1/100 s 6400        Aperture Shutter Speed ISO Image     ƒ/1.8 1/80 s 100    ƒ/4 1/20 s 100    f/8 1/4 s 100    f/14 0.8 s 100    f/22 2.0 s 100    f/22 1.3 s 250    f/22 1/2 s 640    f/22 1/5 s 1600    f/22 1/10 s 3200    f/22 1/25 s 8000    f/8 1/100 s 6400     My favorite here is definitely the ƒ/14, 0.8s, ISO 100. I can hardly spot a difference to the ƒ/22, but 0.8s vs 2s can make a difference if the wind hits your tripod (remember, this is Hamburg!). Also turning up the ISO is no option if your aiming for sharp images.\nSecond motif I did the same series with a second motif and slightly different settings (since the lighting was different):\n   Aperture Shutter Speed ISO Image     ƒ/1.8 1/80 s 100    ƒ/3.5 1/30 s 100    f/8 1/6 s 100    f/13 0.4 s 100    f/22 1.6 s 100    f/22 0.4 s 400    f/22 1/5 s 800    f/22 1/10 s 1600    f/22 1/20 s 3200    f/22 1/40 s 6400    f/8 1/160 s 3200        Aperture Shutter Speed ISO Image     ƒ/1.8 1/80 s 100    ƒ/3.5 1/30 s 100    f/8 1/6 s 100    f/13 0.4 s 100    f/22 1.6 s 100    f/22 0.4 s 400    f/22 1/5 s 800    f/22 1/10 s 1600    f/22 1/20 s 3200    f/22 1/40 s 6400    f/8 1/160 s 3200        Aperture Shutter Speed ISO Image     ƒ/1.8 1/80 s 100    ƒ/3.5 1/30 s 100    f/8 1/6 s 100    f/13 0.4 s 100    f/22 1.6 s 100    f/22 0.4 s 400    f/22 1/5 s 800    f/22 1/10 s 1600    f/22 1/20 s 3200    f/22 1/40 s 6400    f/8 1/160 s 3200     Here the biggest “leap” occurs when going from ƒ/1.8 to ƒ/3.5. From then on the major change is how smooth (or blurred) the waves are. And again, ISO above 200 is not a great choice. Although I am very impressed by the last image at ƒ/8, 1/160s and ISO 3200. That is something you can do without a tripod!\nAnd now, finally, the edited version of the sharpest image:\n\n","permalink":"https://www.jannikarndt.de/blog/2017/11/what_settings_yield_the_sharpest_landscape_photo/","summary":"\u003cp\u003eI was quite surprised at how difficult it is to get \u003cem\u003eabsolutely\u003c/em\u003e razor sharp images out of a very good (high-end-consumer/semi-pro) DSLR camera, namely the Nikon D7100. So I made the test: What settings in terms of aperture, shutter speed and ISO yield the sharpest landscape image?\u003c/p\u003e","title":"What settings yield the sharpest landscape photo?"},{"content":"\n\n\nShot on iPhone 8.\n","permalink":"https://www.jannikarndt.de/blog/2017/11/hamburg_as_seen_from_the_michel/","summary":"\u003cp\u003e\u003ca href=\"/blog/2017/11/hamburg_vom_michel_aus_1.jpg\"\u003e\u003cimg src=\"/blog/2017/11/hamburg_vom_michel_aus_1.jpg\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"Hamburg as seen from the Michel"},{"content":"If you write Scala in a Java-centric environment, chances are you might wind up with a Maven project, defined in a pom.xml. While this can work, it brings a few extra caveats if you want to submit your project to Maven Central (aka Sonatype OSSRH).\nStep 1: Claim your Group ID When you first publish something to the Sonatype OSSRH, you have to\n create a JIRA account and then create a ticket for a new project.  The first question that is usually asked in the ticket is\n Do you own the domain [your group id]? If not, please read: http://central.sonatype.org/pages/choosing-your-coordinates.html\n You can speed up the process by stating that you do own it in the description. Here is an example: https://issues.sonatype.org/browse/OSSRH-34281\nAbout three minutes (or up to 2 business days) later, you have your own repository!\nStep 2: Deploy your first Snapshot  This is a condensed version of the official doc.\n All you need to deploy your first snapshot is to\n add the snapshot repository to the distributionManagement section of your pom.xml:  \u0026lt;distributionManagement\u0026gt; \u0026lt;snapshotRepository\u0026gt; \u0026lt;id\u0026gt;ossrh\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://oss.sonatype.org/content/repositories/snapshots\u0026lt;/url\u0026gt; \u0026lt;/snapshotRepository\u0026gt; \u0026lt;/distributionManagement\u0026gt; add the nexus-staging-maven-plugin to your pom.xml:  \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.sonatype.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;nexus-staging-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.6.7\u0026lt;/version\u0026gt; \u0026lt;extensions\u0026gt;true\u0026lt;/extensions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;serverId\u0026gt;ossrh\u0026lt;/serverId\u0026gt; \u0026lt;nexusUrl\u0026gt;https://oss.sonatype.org/\u0026lt;/nexusUrl\u0026gt; \u0026lt;autoReleaseAfterClose\u0026gt;true\u0026lt;/autoReleaseAfterClose\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; add your credentials to your settings.xml (on macOS in ~/.m2/settings.xml):  \u0026lt;settings\u0026gt; \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;ossrh\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;your-jira-id\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;your-jira-pwd\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;/settings\u0026gt; Since you don\u0026rsquo;t want your password to be written in clear text, you should generate a User Token. Head to https://oss.sonatype.org/#profile and choose \u0026ldquo;User Token\u0026rdquo; in the dropdown menu. Make sure the version in your pom.xml ends in -SNAPSHOT and then  $ mvn clean deploy Et voilà! A few seconds later your snapshot is deployed. Now let\u0026rsquo;s get to the hard part…\nStep 3: Deploy your first Release The requirements for a release are a bit tougher than those for a snapshot.\nStep 3a: Beef up your pom.xml  The version in the pom.xml may not end in -SNAPSHOT The pom.xml may not contain any snapshot-dependencies The pom.xml must contain the name, description, url, license, developer and scm tags, for example:  \u0026lt;name\u0026gt;DataMover\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Out of the box scheduling, logging, monitoring and data governance.\u0026lt;/description\u0026gt; \u0026lt;url\u0026gt;https://github.com/JannikArndt/DataMover\u0026lt;/url\u0026gt; \u0026lt;licenses\u0026gt; \u0026lt;license\u0026gt; \u0026lt;name\u0026gt;MIT License\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://www.opensource.org/licenses/mit-license.php\u0026lt;/url\u0026gt; \u0026lt;/license\u0026gt; \u0026lt;/licenses\u0026gt; \u0026lt;developers\u0026gt; \u0026lt;developer\u0026gt; \u0026lt;name\u0026gt;Jannik Arndt\u0026lt;/name\u0026gt; \u0026lt;email\u0026gt;jannik@jannikarndt.de\u0026lt;/email\u0026gt; \u0026lt;/developer\u0026gt; \u0026lt;/developers\u0026gt; \u0026lt;scm\u0026gt; \u0026lt;connection\u0026gt;scm:git:git://github.com/JannikArndt/DataMover.git\u0026lt;/connection\u0026gt; \u0026lt;developerConnection\u0026gt;scm:git:ssh://github.com:JannikArndt/DataMover.git \u0026lt;/developerConnection\u0026gt; \u0026lt;url\u0026gt;https://github.com/JannikArndt/DataMover/tree/master\u0026lt;/url\u0026gt; \u0026lt;/scm\u0026gt;  And you may need the staging repository in the distributionManagement section:  \u0026lt;distributionManagement\u0026gt; \u0026lt;snapshotRepository\u0026gt; \u0026lt;id\u0026gt;ossrh\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://oss.sonatype.org/content/repositories/snapshots\u0026lt;/url\u0026gt; \u0026lt;/snapshotRepository\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;ossrh\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/distributionManagement\u0026gt; (there are so many different ways to configure the repository that I\u0026rsquo;m not really sure about this)\nStep 3b and 3c: Create sources and javadoc You must upload a …-javadoc.jar and …-sources.jar with the ….pom and the ….jar. This is a bit difficult if you write Scala code, so there used to be the option to disable this requirement on a per-project basis. The currently suggested solution is to fake it:\n If, for some reason (for example, license issue or it\u0026rsquo;s a Scala project), you can not provide -sources.jar or -javadoc.jar , please make fake -sources.jar or -javadoc.jar with simple README inside to pass the checking. We do not want to disable the rules because some people tend to skip it if they have an option and we want to keep the quality of the user experience as high as possible.\n However, being a scala dev you probably don\u0026rsquo;t fancy faking anything. scala-maven-plugin to the rescue! In the build section of your pom.xml, you probably already have used the scala-maven-plugin to compile your project. Extend it with the following executions:\n\u0026lt;plugin\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/net.alchim31.maven/scala-maven-plugin --\u0026gt; \u0026lt;groupId\u0026gt;net.alchim31.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;scala-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.2\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;compile\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;!-- from here --\u0026gt; \u0026lt;id\u0026gt;attach-javadocs\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;doc-jar\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;attach-sources\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;add-source\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;!-- to here --\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; Now you don\u0026rsquo;t even need the maven-release-plugin, maven-source-plugin or maven-javadoc-plugin anymore!\nLet\u0026rsquo;s head for the dragon…\nStep 3d: Sign your name across my heart your deployables  Warning: This section contains dirty workarounds. Nothing else.\n You need to provide a valid gpg-signature for the …-javadoc.jar, …-sources.jar, ….pom and the ….jar. This means these files may not change between signing and deployment. If you create them (via mvn clean install), sign them and then try to mvn deploy them, however, they will change.\nNow you can try one of the many other ways to deploy, but we\u0026rsquo;ve already got so far, with javadoc and sources and everything, let\u0026rsquo;s not throw this all away!\n “Vorwärts immer, rückwärts nimmer.”\n(Carl Latann, unlucky musician only know for being cited by Erich “Fowler” Honecker)\n First things first: Add the maven-gpg-plugin to your pom.xml:\n\u0026lt;plugin\u0026gt; \u0026lt;!-- GPG signing --\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-gpg-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.6\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;sign-artifacts\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;verify\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;sign\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; This is supposedly running in the verify phase of the maven lifecycle. Or not. It means, though, that you can invoke mvn gpg:sign to create the needed .asc files.\nThis assumes that you have gpg installed and the gpg-agent is running. Also, you need to provide the password to your key —devil or deep blue sea— either as a console argument or in plain text in your settings.xml:\n\u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;ossrh\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;gpg.executable\u0026gt;gpg2\u0026lt;/gpg.executable\u0026gt; \u0026lt;gpg.passphrase\u0026gt;iShouldNotBeInPlainText\u0026lt;/gpg.passphrase\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; The manual claims that you could encrypt the passphrase, but that\u0026rsquo;s just a lie. I have spend a lot of time on this, but no success. If anyone manages to read an encrypted passphrase, please let me know!\nWell, this was bad. Let\u0026rsquo;s go for the big finale…\nStep 3e: Actually deploy Okay, usually a mvn deploy goes through the complete lifecycle and does all the required steps, but we\u0026rsquo;ve added a few non-standard steps, so…well…this works:\nmvn clean package source:jar gpg:sign install:install deploy:deploy But wait, there\u0026rsquo;s more!\nStep 3f: Release Head over to https://oss.sonatype.org/#stagingRepositories and (if your prefix starts with anything after \u0026lsquo;c\u0026rsquo;) scroll to the bottom, where you find your repository!\nThe naming is a bit weird here: The repository is now open, which means you can continue deploying into it. To move forward you have to close it. This is where you\u0026rsquo;ll get problems if you skipped a step (I told you so!).\nIf everything worked, you should see something like this:\nAnd now you can finally Release your work! Enjoy your work, give yourself a pat on the shoulder and then head back to your code, increase the version to the next -SNAPSHOT and start fixing those bugs you know are still in there…\nOh, and don\u0026rsquo;t forget to comment on the ticket, otherwise nobody will find your code:\n","permalink":"https://www.jannikarndt.de/blog/2017/09/releasing_a_scala_maven_project_to_maven_central/","summary":"If you write Scala in a Java-centric environment, chances are you might wind up with a Maven project, defined in a pom.xml. While this can work, it brings a few extra caveats if you want to submit your project to Maven Central (aka Sonatype OSSRH).\nStep 1: Claim your Group ID When you first publish something to the Sonatype OSSRH, you have to\n create a JIRA account and then create a ticket for a new project.","title":"Releasing a Scala + Maven project to Maven Central (Sonatype OSS)"},{"content":"Storing case classes in a MongoDB database is incredibly easy, once you know how. The same goes for java.time classes such as ZonedDateTime, LocalDate or Duration.\nThis example uses the official Mongo Scala Driver in version 2.x and the bsoncodec project by Ralph Schaer.\nThe solution First: A complete working example:\nimport java.math.BigDecimal import java.time.{LocalDate, LocalDateTime} import scala.concurrent.Await import scala.concurrent.duration._ import scala.language.postfixOps object MongoDbExample { def main(args: Array[String]): Unit = { val myParcel = Parcel( sender = Address(\u0026#34;Online Shop 3000\u0026#34;, \u0026#34;E-Commerce-Street\u0026#34;, \u0026#34;Berlin\u0026#34;), receiver = Address(\u0026#34;Jannik\u0026#34;, \u0026#34;Fun-Street\u0026#34;, \u0026#34;Hamburg\u0026#34;), sent = LocalDate.of(2017, 8, 12), received = Some(LocalDateTime.of(2017, 8, 19, 10, 24)), contents = Seq(Content(\u0026#34;Cool Robot\u0026#34;, new BigDecimal(\u0026#34;39.99\u0026#34;)), Content(\u0026#34;Drone\u0026#34;, new BigDecimal(\u0026#34;199.99\u0026#34;))) ) Await.result(DB.parcels.insertOne(myParcel).toFuture, 10 seconds) val allParcels = Await.result(DB.parcels.find().toFuture(), 10 seconds) allParcels.foreach(println) } } case class Parcel(sender: Address, receiver: Address, sent: LocalDate, received: Option[LocalDateTime], contents: Seq[Content]) case class Address(name: String, street: String, city: String) case class Content(name: String, price: BigDecimal) object DB { import ch.rasc.bsoncodec.math._ import ch.rasc.bsoncodec.time._ import org.bson.codecs.configuration.CodecRegistries import org.bson.codecs.configuration.CodecRegistries._ import org.mongodb.scala.bson.codecs.DEFAULT_CODEC_REGISTRY import org.mongodb.scala.bson.codecs.Macros._ import org.mongodb.scala.{MongoClient, MongoCollection, MongoDatabase} private val customCodecs = fromProviders(classOf[Parcel], classOf[Address], classOf[Content]) private val javaCodecs = CodecRegistries.fromCodecs( new LocalDateTimeDateCodec(), new LocalDateDateCodec(), new BigDecimalStringCodec()) private val codecRegistry = fromRegistries(customCodecs, javaCodecs, DEFAULT_CODEC_REGISTRY) private val database: MongoDatabase = MongoClient().getDatabase(\u0026#34;TrackingData\u0026#34;) .withCodecRegistry(codecRegistry) val parcels: MongoCollection[Parcel] = database.getCollection(\u0026#34;Parcels\u0026#34;) } This results in the following entry:\n{ \u0026#34;_id\u0026#34; : ObjectId(\u0026#34;5997f9ff96ddbad1d424981d\u0026#34;), \u0026#34;sender\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;Online Shop 3000\u0026#34;, \u0026#34;street\u0026#34; : \u0026#34;E-Commerce-Street\u0026#34;, \u0026#34;city\u0026#34; : \u0026#34;Berlin\u0026#34; }, \u0026#34;receiver\u0026#34; : { \u0026#34;name\u0026#34; : \u0026#34;Jannik\u0026#34;, \u0026#34;street\u0026#34; : \u0026#34;Fun-Street\u0026#34;, \u0026#34;city\u0026#34; : \u0026#34;Hamburg\u0026#34; }, \u0026#34;sent\u0026#34; : ISODate(\u0026#34;2017-08-12T00:00:00.000Z\u0026#34;), \u0026#34;received\u0026#34; : ISODate(\u0026#34;2017-08-19T10:24:00.000Z\u0026#34;), \u0026#34;contents\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;Cool Robot\u0026#34;, \u0026#34;price\u0026#34; : \u0026#34;39.99\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;Drone\u0026#34;, \u0026#34;price\u0026#34; : \u0026#34;199.99\u0026#34; } ] } How to get there Adding codecs for custom classes The magic is obviously hidden in the DB object. Following just the driver documentation, one might start with this simpler version:\nobject DB { import org.mongodb.scala.{MongoClient, MongoCollection, MongoDatabase} private val database: MongoDatabase = MongoClient().getDatabase(\u0026#34;TrackingData\u0026#34;) val parcels: MongoCollection[Parcel] = database.getCollection(\u0026#34;Parcels\u0026#34;) } This results in the exception\nException in thread \u0026quot;main\u0026quot; org.bson.codecs.configuration.CodecConfigurationException: Can't find a codec for class de.jannikarndt.MongoDbExample.Parcel. Googleing for the problem, you find quite many outdated solutions, such as salat, which is still maintained but uses the outdated casbah driver (aka \u0026ldquo;version 1\u0026rdquo;).\nAlex Landa explains, that version 2 of the driver fixes this problem with macros:\n The Codec class is generated during the compile time and then added to the default codec registry (using the fromRegistries method).\n So the next step is to create codecs for all custom classes and tell the MongoClient to use these codecs:\nobject DB { import org.bson.codecs.configuration.CodecRegistries._ import org.mongodb.scala.bson.codecs.DEFAULT_CODEC_REGISTRY import org.mongodb.scala.bson.codecs.Macros._ import org.mongodb.scala.{MongoClient, MongoCollection, MongoDatabase} private val customCodecs = fromProviders(classOf[Parcel], classOf[Address], classOf[Content]) private val codecRegistry = fromRegistries(customCodecs, DEFAULT_CODEC_REGISTRY) private val database: MongoDatabase = MongoClient().getDatabase(\u0026#34;TrackingData\u0026#34;) .withCodecRegistry(codecRegistry) val parcels: MongoCollection[Parcel] = database.getCollection(\u0026#34;Parcels\u0026#34;) } Adding codecs for system classes This brings us one step further, to the next exception:\nException in thread \u0026quot;main\u0026quot; org.bson.codecs.configuration.CodecConfigurationException: Can't find a codec for class java.time.LocalDate. If you try to add classOf[LocalDate] to the list, you get a compile error, stating that\nError:(41, 106) java.time.LocalDate does not have a constructor private val customCodecs = fromProviders(classOf[Parcel], classOf[Address], classOf[Content], classOf[LocalDate]) You (or a macro you invoke) cannot add code to the standard java classes. At this point, you would have to write a codec yourself, extending / implementing org.bson.codecs.Codec and overriding encode and decode.\nLuckily, Ralph Schaer has already done this — not just for LocalDate but for most of java.time, java.sql, URLs, BigDecimal, javax.money, and more, and for many types you can even decide how to encode them, i.e. to date, document or string.\nThat\u0026rsquo;s what the lines\nprivate val javaCodecs = CodecRegistries.fromCodecs( new LocalDateTimeDateCodec(), new LocalDateDateCodec(), new BigDecimalStringCodec()) private val codecRegistry = fromRegistries(customCodecs, javaCodecs, DEFAULT_CODEC_REGISTRY) in the final solution (see top of the page) are about.\n","permalink":"https://www.jannikarndt.de/blog/2017/08/writing_case_classes_to_mongodb_in_scala/","summary":"\u003cp\u003eStoring case classes in a MongoDB database is incredibly easy, once you know how. The same goes for \u003ccode\u003ejava.time\u003c/code\u003e classes such as \u003ccode\u003eZonedDateTime\u003c/code\u003e, \u003ccode\u003eLocalDate\u003c/code\u003e or \u003ccode\u003eDuration\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThis example uses the official \u003ca href=\"https://mvnrepository.com/artifact/org.mongodb.scala/mongo-scala-driver_2.11\"\u003eMongo Scala Driver\u003c/a\u003e in version 2.x and the \u003ca href=\"https://github.com/ralscha/bsoncodec\"\u003ebsoncodec\u003c/a\u003e project by \u003ca href=\"https://github.com/ralscha\"\u003eRalph Schaer\u003c/a\u003e.\u003c/p\u003e","title":"Writing case classes to MongoDB in Scala"},{"content":" This post contains the absolut essence from the Terraform Getting Started Guide: https://www.terraform.io/intro/getting-started/install.html\n Preparation brew install terraform Create aws.tf:\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-2757f631\u0026#34; // https://cloud-images.ubuntu.com/locator/ec2/ instance_type = \u0026#34;t2.micro\u0026#34; } AMI = Amazon Machine Images\nAWS credentials are stored in environment vars:\nexport AWS_ACCESS_KEY_ID=... export AWS_SECRET_ACCESS_KEY=... Init $ terraform init ... Terraform has been successfully initialized! ... $ terraform plan -out planfile + aws_instance.example ami: \u0026#34;ami-2757f631\u0026#34; associate_public_ip_address: \u0026#34;\u0026lt;computed\u0026gt;\u0026#34; ... $ terraform apply planfile aws_instance.example: Creating... ... aws_instance.example: Still creating... (10s elapsed) aws_instance.example: Still creating... (20s elapsed) aws_instance.example: Still creating... (30s elapsed) aws_instance.example: Still creating... (40s elapsed) aws_instance.example: Creation complete (ID: i-00b2e1a29daee4371) $ terraform show aws_instance.example: id = i-00b2e1a29daee4371 ami = ami-2757f631 associate_public_ip_address = true Change ami-2757f631 (Ubuntu 16.04 LTS AMI) =\u0026gt; ami-b374d5a5 (Ubuntu 16.10 AMI)\nIn aws.tf:\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-b374d5a5\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } $ terraform plan Refreshing Terraform state in-memory prior to plan... aws_instance.example: Refreshing state... (ID: i-00b2e1a29daee4371) ... -/+ aws_instance.example (new resource required) ami: \u0026#34;ami-2757f631\u0026#34; =\u0026gt; \u0026#34;ami-b374d5a5\u0026#34; (forces new resource) associate_public_ip_address: \u0026#34;true\u0026#34; =\u0026gt; \u0026#34;\u0026lt;computed\u0026gt;\u0026#34; ... $ terraform apply aws_instance.example: Refreshing state... (ID: i-00b2e1a29daee4371) aws_instance.example: Destroying... (ID: i-00b2e1a29daee4371) ... aws_instance.example: Destruction complete aws_instance.example: Creating... ami: \u0026#34;\u0026#34; =\u0026gt; \u0026#34;ami-b374d5a5\u0026#34; associate_public_ip_address: \u0026#34;\u0026#34; =\u0026gt; \u0026#34;\u0026lt;computed\u0026gt;\u0026#34; ... aws_instance.example: Creation complete (ID: i-05a8f2e88ae0faace) Destroy $ terraform plan -destroy Refreshing Terraform state in-memory prior to plan... ... - aws_instance.example ... $ terraform destroy aws_instance.example: Refreshing state... (ID: i-05a8f2e88ae0faace) ... - aws_instance.example Terraform will delete all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value: yes aws_instance.example: Destroying... (ID: i-05a8f2e88ae0faace) aws_instance.example: Destruction complete Destroy complete! Resources: 1 destroyed. Assigning an IP to the instance In aws.tf:\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-b374d5a5\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } resource \u0026#34;aws_eip\u0026#34; \u0026#34;ip\u0026#34; { instance = \u0026#34;${aws_instance.example.id}\u0026#34; } $ terraform apply planfile aws_instance.example: Creating... ... aws_instance.example: Creation complete (ID: i-0478b3c5b6a085287) aws_eip.ip: Creating... ... aws_eip.ip: Creation complete (ID: eipalloc-6be79a58) Apply complete! Resources: 2 added, 0 changed, 0 destroyed. $ terraform refresh aws_instance.example: Refreshing state... (ID: i-0478b3c5b6a085287) aws_eip.ip: Refreshing state... (ID: eipalloc-6be79a58) $ terraform show aws_eip.ip: id = eipalloc-6be79a58 association_id = eipassoc-590d9e6c domain = vpc instance = i-0478b3c5b6a085287 network_interface = eni-73845aa6 private_ip = 172.31.17.98 public_ip = 52.204.255.194 vpc = true aws_instance.example: id = i-0478b3c5b6a085287 ami = ami-b374d5a5 associate_public_ip_address = true Dependencies The order in which resources are created is determined by terraforms analysis of (implicit) dependencies. Dependencies can also be defined explicitly, for example:\nresource \u0026#34;aws_eip\u0026#34; \u0026#34;ip\u0026#34; { instance = \u0026#34;${aws_instance.example.id}\u0026#34; depends_on = [\u0026#34;aws_instance.example\u0026#34;] # \u0026lt;- explicit dependency } To view a dependency graph:\n$ terraform graph \u0026gt; graph.dot And open with graphviz: aws_eip.ip   aws_instance.example    provider.aws    [root] meta.count-boundary (count boundary fixup)    [root] provider.aws (close)    [root] root      \nProvisioning/Bootstrapping  https://www.terraform.io/docs/provisioners/index.html\n Change aws.tf:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-b374d5a5\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; # add provisioner provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo ${aws_instance.example.public_ip} \u0026gt; ip_address.txt\u0026#34; } } # add output output \u0026#34;ip\u0026#34; { value = \u0026#34;${aws_eip.ip.public_ip}\u0026#34; } Provisioners are only run, when a resource is first created!\n$ terraform destroy ... $ terraform apply ... Outputs: ip = 50.17.232.209 ","permalink":"https://www.jannikarndt.de/blog/2017/08/very_quick_start_terraform/","summary":"\u003cblockquote\u003e\n\u003cp\u003eThis post contains the absolut essence from the Terraform Getting Started Guide:\n\u003ca href=\"https://www.terraform.io/intro/getting-started/install.html\"\u003ehttps://www.terraform.io/intro/getting-started/install.html\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","title":"VERY Quick Start: Terraform"},{"content":"Ever wondered where bad code comes from?\n “This story is done”\n  “Shouldn’t someone review it first?”\n  “Oh, yeah … erm … I’ll do a quick refactoring first and then…”\n …when that other person is on holiday!\n","permalink":"https://www.jannikarndt.de/blog/2017/07/where_bad_code_comes_from/","summary":"\u003cp\u003eEver wondered where bad code comes from?\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“This story is done”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“Shouldn’t someone review it first?”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e“Oh, yeah … erm … I’ll do a quick refactoring first and then…”\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e…when that other person is on holiday!\u003c/p\u003e","title":"Where bad code comes from"},{"content":"Some ideas seem great at first but turn out to be incredibly bad in hindsight. I reviewed such an idea today.\nBackground The task was to store flight information in a database. The information is transmitted via the IATA SSIM format, which standardizes messages with about 70 different fields. There also are various different types of messages, all of which might contain any of the 70 fields. An example: Airlines can publish their Standard Schedules Messages which contain the seasonal flight schedule, i.e. routes they usually fly on which days of the week. Exceptions from this get their own message (using different fields). Unforeseen changes are published via the Ad hoc Schedules Messages which, of course, use different fields as well. And then there are Movement Messages (from the Airport Handling Manual AHM) which contain the actual information on what each plane does. All of these messages make up a flight: The schedules, the ad hoc changes and what actually happened. The contents overlap but rarely match. In object oriented thinking, this would be extremely easy: You have an abstract message class, implementations for SSM, ASM and MVT and further classes that extend each of these for special cases.\nEfficiently storing this in a database however is not that easy. You\u0026rsquo;re bound to wind up with a horrifying amount of columns and an awful lot of null entries. The contents will resemble the specification which lists all the elements each message sub-type must or might contain:\nMESSAGES    message_id sender_id foo boo tip bli bla blubb     12 9 \u0026lsquo;bar\u0026rsquo; null \u0026lsquo;top\u0026rsquo; null null null   13 9 null null null 1 2 42    Why not…? Yes, this structure is the reason someone invented document-oriented databases. However it is the only information that would require us to leave the relational world and operate yet another service. Also, the contents are tightly integrated into a lot of other tables, and we love referential integrity.\nStoring key-value-pairs in a relational database You can store key-value-pairs in a database, for example like this:\nMESSAGES    id sender_id     12 9   13 9    VALUES_TABLE    id key value message_id     1 \u0026lsquo;foo\u0026rsquo; \u0026lsquo;bar\u0026rsquo; 12   2 \u0026lsquo;tip\u0026rsquo; \u0026lsquo;top\u0026rsquo; 12    Now in our case the keys are well defined, so you will have a lot of redundancies there. The idea I was reviewing extracted the keys into yet another table:\nMESSAGES    id sender_id     12 9   13 9    VALUES_TABLE    id key_id value message_id     1 1 \u0026lsquo;bar\u0026rsquo; 12   2 3 \u0026lsquo;tip\u0026rsquo; 12    and\nKEYS_TABLE    key_id key     1 \u0026lsquo;foo\u0026rsquo;   2 \u0026lsquo;boo\u0026rsquo;   3 \u0026lsquo;tip\u0026rsquo;   4 \u0026lsquo;bli\u0026rsquo;   5 \u0026lsquo;bla\u0026rsquo;   6 \u0026lsquo;blubb\u0026rsquo;    Wonderfull! No nulls, no redundancies, no more than four columns! This idea sounds great!\n…and hell breaks loose Database design fails can live in production for a long time until they get noticed. And that\u0026rsquo;s when all your precious data is already put into this bad scheme.\nIn the case of the above solution things get ugly the moment you query for something specific. For example all messages where \u0026lsquo;foo\u0026rsquo; is \u0026lsquo;bar\u0026rsquo;:\nSELECT * FROM MESSAGES m JOIN VALUES_TABLE v ON m.id = v.message_id JOIN KEYS_TABLE k ON v.key_id = k.key_id WHERE k.key = \u0026#39;foo\u0026#39; AND v.value = \u0026#39;bar\u0026#39; As opposed to\nSELECT * FROM MESSAGES m WHERE m.foo = \u0026#39;bar\u0026#39; The problem only grows when you create this query from your business code: You have to get the \u0026lsquo;foo\u0026rsquo; string from somewhere:\n You might hard code it. Yak. You might create an enum replicating the KEYS_TABLE. Until that table changes—and beware, not the structure, but the content! You might be extra clever and save a JOINby using the key_id directly!  My best guess is that you\u0026rsquo;d wind up with all three options scattered in your code. Good look.\nThe solution / workaround As I said in the beginning, tables with many columns and null entries might not be very elegant, but databases and developers are used to them. Also, document-oriented databases were invented for a reason. This might just be that reason.\nThere is another option: vertical partitioning. Especially in this use case, where there is a specification that provides the information on where to split:\nMESSAGES    id sender_id     12 9   13 9    MESSAGES_PART_1    id foo tip     12 \u0026lsquo;bar\u0026rsquo; \u0026lsquo;top\u0026rsquo;    MESSAGES_PART_2    id bli bla blubb     13 1 2 42    Let\u0026rsquo;s check for our query:\nSELECT * FROM MESSAGES m LEFT JOIN MESSAGES_PART_1 p1 ON m.id = p1.id LEFT JOIN MESSAGES_PART_2 p2 ON m.id = p2.id WHERE p1.foo = \u0026#39;bar\u0026#39; Well… it doesn\u0026rsquo;t suck as much. However, if you\u0026rsquo;re on the INSERT side of the database you\u0026rsquo;re gonna have a bad time…\n","permalink":"https://www.jannikarndt.de/blog/2017/07/storing_sparse_key_value_like_data_in_a_relational_database/","summary":"\u003cp\u003eSome ideas seem great at first but turn out to be incredibly bad in hindsight. I reviewed such an idea today.\u003c/p\u003e","title":"Storing sparse, key-value-like data in a relational database"},{"content":"","permalink":"https://www.jannikarndt.de/blog/2017/07/tough_mudder_norddeutschland_2017/","summary":"\u003cimg src=\"/blog/2017/07/tough_mudder/tough_mudder_1.jpg\" alt=\"\"\u003e","title":"Tough Mudder Norddeutschland 2017"},{"content":"","permalink":"https://www.jannikarndt.de/blog/2017/07/a_minor_story_live_at_methfesselfest/","summary":"\u003ciframe width=\"100%\" height=\"400\" src=\"https://www.youtube-nocookie.com/embed/XjWi6jnxxNU?rel=0\u0026amp;showinfo=0\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e","title":"A Minor Story Live at Methfesselfest"},{"content":"I tried to create a single image that contains all the most important git commands:\nThe source code is available at https://github.com/JannikArndt/git-in-one-image and is licensed under a Creative Commons Attribution 4.0 International License.\n","permalink":"https://www.jannikarndt.de/blog/2017/06/git_in_one_image/","summary":"\u003cp\u003eI tried to create a single image that contains all the most important \u003ccode\u003egit\u003c/code\u003e commands:\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://rawgit.com/JannikArndt/git-in-one-image/master/git-in-one-image.svg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Git in one Image"},{"content":"The Problem You\u0026rsquo;re a corporation. Your IT department is old, slow and can\u0026rsquo;t innovate. Your competitor however can. So you try what every corporation tries: Two-speed IT.\nThe solution It\u0026rsquo;s easy: You take the best employees from every department, put them together, give them the highest priority and a lot of funding. Kind of like a startup, but with corporate money.\nThe catch You\u0026rsquo;re a corporation. You don\u0026rsquo;t have good employees. You drove them away years ago, with your processes and hierarchies. You only have employees who suck less then the others.\nThe solution You hire a bunch of consultants, so your people can tell them to innovate.\nThe catch You\u0026rsquo;re a corporation, remember? You don\u0026rsquo;t have good employees. You have people who are good with processes and hierarchies. They will inevitably build the only thing they know: An old, slow department that can\u0026rsquo;t innovate, with processes, hierarchies and consultants. Also, you have deprived your actual IT department of its best employees.\n","permalink":"https://www.jannikarndt.de/blog/2017/05/two_speed_it/","summary":"\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eYou\u0026rsquo;re a corporation. Your IT department is old, slow and can\u0026rsquo;t innovate. Your competitor however can. So you try what every corporation tries: Two-speed IT.\u003c/p\u003e","title":"Two-speed IT"},{"content":"Last week one of our programs failed looking up an airplane by its registration. That\u0026rsquo;s not a surprise, since ac regs are a horrible identifier. They change all the time. Also there is almost no naming rule at all. Wikipedia states\n When painted on the fuselage, the prefix and suffix are usually separated by a dash (for example, YR-BMA). When entered in a flight plan, the dash is omitted (for example, YRBMA). In some countries that use a number suffix rather than letters, like the United States (N), South Korea (HL), and Japan (JA), the prefix and suffix are connected without a dash.\n Okay, so the only thing in common is a prefix and a main part, separated by a dash, but that\u0026rsquo;s often dropped. Nice! Also the both parts might consist of letters and numbers. And they vary in length. And most of the time, the prefix designates the country of registration, although there are exceptions, for example Guernsey, which is not a part of Great Britain (G–) but its property, therefore having the registration 2– They island has its own airline, Aurigny, but all of their 10 aircraft use the G--Prefix.\n(https://en.wikipedia.org/wiki/Aurigny#/media/File:G-JOEY.jpg) And then there are military aircraft. Since they are not affected by the Chicago Convention on International Civil Aviation, they are free to do whatever they want. In the US, they use the base code (as in code of the military base), year they were ordered and a serial number. In Germany consists of the aircraft type and a serial number. And this is where my error comes from:\nWe recently switched one of our source systems, and apparently the aircraft formerly entered as 10-27 now is 10+27. That\u0026rsquo;s weird, because the old source is more trustworthy (but incomplete), and you usually use dashes, not plus-signs.\nA quick Google-search turns up the wikipedia page and an official Luftwaffe page, both in favour of the plus. Okay, but this is Germany, there surely is an official Behörde that clearly regulates all this! — Yes, there is! It\u0026rsquo;s the Luftfahrzeugrolle and their website states that due to data protection reasons they cannot disclose anything. Marvellous! The information written in huge letters and visible for everyone is protected. Wait! It\u0026rsquo;s written on the plane! And there are a lot of people taking pictures of airplanes! Alas, the solution is:\n(https://commons.wikimedia.org/wiki/File:Luftwaffe_A310_10%2B27.jpg) They use the iron cross as a separator! I give up…\n","permalink":"https://www.jannikarndt.de/blog/2017/05/the_real_world_doesnt_care_about_encoding/","summary":"\u003cp\u003eLast week one of our programs failed looking up an airplane by its \u003ca href=\"https://en.wikipedia.org/wiki/Aircraft_registration\"\u003eregistration\u003c/a\u003e. That\u0026rsquo;s not a surprise, since ac regs are a \u003cem\u003ehorrible\u003c/em\u003e identifier. They change all the time. Also there is almost no naming rule at all. Wikipedia states\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhen painted on the fuselage, the prefix and suffix are usually separated by a dash (for example, \u003ccode\u003eYR-BMA\u003c/code\u003e). When entered in a flight plan, the dash is omitted (for example, \u003ccode\u003eYRBMA\u003c/code\u003e). In some countries that use a number suffix rather than letters, like the United States (\u003ccode\u003eN\u003c/code\u003e), South Korea (\u003ccode\u003eHL\u003c/code\u003e), and Japan (\u003ccode\u003eJA\u003c/code\u003e), the prefix and suffix are connected without a dash.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"The real world doesn't care about encoding"},{"content":"My favourite animal: The Beluga!\n","permalink":"https://www.jannikarndt.de/blog/2017/04/belugas_at_finkenwerder/","summary":"\u003cp\u003eMy favourite animal: The Beluga!\u003c/p\u003e\n\u003cimg src=\"/blog/2017/04/Beluga_1.jpg\" alt=\"\"\u003e","title":"Belugas at Finkenwerder"},{"content":"TL;DR: You don\u0026rsquo;t. We eventually gave up on it.\nMy personal lessons-learned:\n Pentaho Kettle (or “Community Edition”, CE, i.e. the open-source core) is a great product for one-time data transfer or on-demand data transfer, but not for resilient, scheduled jobs. The “Enterprise Edition” (EE) adds scheduling that doesn\u0026rsquo;t work reliably, and a very powerless server. Kerberos is a bitch.  The requirements We were looking for a system to write, deploy and schedule ETL jobs. Since we actually want to move the data, the en-vogue trend of NoETL and queries on source systems doesn\u0026rsquo;t work for us. After giving up on talend, Pentaho made it quite easy to write the kind of jobs we need. However, making sure these jobs can be deployed to a server and reliably run there turn out as almost impossible.\nWhat I want:\n Deployment in one click (in 2017, Jenkins is nothing new!) Deployment on three development environments (dev, test, prod) Configuration of different schedules for each environment Version control (being able to answer “who changed what when?”)  Solution #1: Export and Import via GUI I was surprised that the official recommendation is exporting and importing the complete repository.\nProblems:\n You cannot deploy single jobs Every user needs to have access to production (no-go!) Nothing tracks what actually changed This needs way more than one click The export does not contain schedules  Solution #2: Export and Import via Shell Script The page also suggest using shell access. This sounds a lot closer to a one-click-solution. You need to have both repositories configured in your application. These settings are stored in ~~/.kettle/repositories.xml and matched by name:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;repositories\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;PentahoEnterpriseRepository\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;MyRepository\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Pentaho repository\u0026lt;/description\u0026gt; \u0026lt;is_default\u0026gt;false\u0026lt;/is_default\u0026gt; \u0026lt;repository_location_url\u0026gt;http\u0026amp;#x3a;\u0026amp;#x2f;\u0026amp;#x2f;localhost\u0026amp;#x3a;8080\u0026amp;#x2f;pentaho \u0026lt;/repository_location_url\u0026gt; \u0026lt;version_comment_mandatory\u0026gt;N\u0026lt;/version_comment_mandatory\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;/repositories\u0026gt; With this configuration, you can export and import repositories via pan.sh and import.sh:\n./pan.sh -rep=MyRepository -user=Jannik -pass=password -dir=home/jannik -exprep=\u0026#34;pentaho_export.xml\u0026#34; -logfile=\u0026#34;export.log\u0026#34; ./import.sh -rep=\u0026#34;Pentaho Repository\u0026#34; -user=Jannik -pass=password -dir=home/import -file=\u0026#34;pentaho_export.xml\u0026#34; -logfile=\u0026#34;export.log\u0026#34; -replace=true -comment=\u0026#34;New Version\u0026#34; -norules=true Tip: The directory name is always in lower case, independent of what the web UI shows you.\nProblems:\n You cannot deploy single jobs Every user needs to have access to production (no-go!) Nothing tracks what actually changed This needs way more than one click The export does not contain schedules  Is that all? No. If you take a look at pan.sh you\u0026rsquo;ll find, what it really does:\n\u0026#34;$DIR/spoon.sh\u0026#34; -main org.pentaho.di.pan.Pan -initialDir \u0026#34;$INITIALDIR/\u0026#34; \u0026#34;$@\u0026#34; The same for import.sh:\n\u0026#34;$DIR/spoon.sh\u0026#34; -main org.pentaho.di.imp.Import \u0026#34;$@\u0026#34; Deployment suddenly means starting the complete Pentaho Suite — twice!\nSolution #3: Export and Import via REST Baffled with why I\u0026rsquo;m not happy with manually ex- and importing, my contact at Pentaho suggested the REST API. If you\u0026rsquo;re still unsure if cyclic dependencies are a bad thing, try reading the API Documentation. And if you need to convince someone that generated documentation might be a bad idea, show him this overview. Spoiler: both fail to mention the actual address of the endpoint, which is http://localhost:8080/pentaho/api/repo/.... Luckily, there are people writing useful blog entries. And if you need a tool for trial-and-error, I recommend Insomnia. Spending a lot of nerves, I crafted this beauty:\n#!/bin/sh  source_url=$1 target_url=$2 echo \u0026#34;\\033[1mDeploying from ${source_url}to ${target_url}\\033[0m\u0026#34; echo \u0026#34;\\033[0;32mExporting backup…\\033[0m\u0026#34; # password encrypted via encr.sh curl -H \u0026#34;Authorization: Basic EncryptedPassword\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -i ${source_url}api/repo/files/backup -o Backup.zip echo \u0026#34;\\033[0;32mUnzipping backup…\\033[0m\u0026#34; unzip -o -q Backup.zip echo \u0026#34;\\033[0;32mDeleting zip-file…\\033[0m\u0026#34; rm Backup.zip echo \u0026#34;\\033[0;32mDeleting home-folder…\\033[0m\u0026#34; rm -r home/ # delete everything you don\u0026#39;t want to deploy # add, commit and push to a git here echo \u0026#34;\\033[0;32mCreating new zip-file for deployment…\\033[0m\u0026#34; zip -r -q Backup.zip \u0026#39;_datasources/\u0026#39; \u0026#39;etc/\u0026#39; \u0026#39;exportManifest.xml\u0026#39; \u0026#39;metastore.mzip\u0026#39; \u0026#39;public/\u0026#39; echo \u0026#34;\\033[0;32mDeploying...\\033[0m\u0026#34; curl -H \u0026#34;Authorization: Basic EncryptedPassword\u0026#34; -H \u0026#34;Content-Type: multipart/form-data\u0026#34; -H \u0026#34;overwrite: true\u0026#34; -F \u0026#34;fileUpload=@Backup.zip\u0026#34; -i ${target_url}api/repo/files/systemRestore echo \u0026#34;\\033[0;32mCleaning up…\\033[0m\u0026#34; rm Backup.zip Let\u0026rsquo;s look at our problem-list:\n You cannot deploy single jobs - but single directories! Every user needs to have access to production (no-go!) (this can be run on a server) Nothing tracks what actually changed (you can automatically commit the xml file, the backup contains one for each job/transaction) This needs way more than one click The export does not contain schedules Pentaho Suite is started twice and takes minutes (this accesses the repository server directly)  Okay, so this sounds pretty good! What\u0026rsquo;s the problem? It doesn\u0026rsquo;t work. The systemRestore endpoint happily takes all your data, answers 200: OK and does nothing. I was told, a ticket would be raised, but so far I haven\u0026rsquo;t seen any of it and the implementation also hasn\u0026rsquo;t changed.\n","permalink":"https://www.jannikarndt.de/blog/2017/03/deploying_pentaho_jobs_into_production/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR:\u003c/strong\u003e You don\u0026rsquo;t. We eventually gave up on it.\u003c/p\u003e\n\u003cp\u003eMy personal lessons-learned:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePentaho Kettle (or “Community Edition”, CE, i.e. the open-source core) is a great product for \u003cem\u003eone-time\u003c/em\u003e data transfer or \u003cem\u003eon-demand\u003c/em\u003e data transfer, but not for resilient, scheduled jobs.\u003c/li\u003e\n\u003cli\u003eThe “Enterprise Edition” (EE) adds scheduling that doesn\u0026rsquo;t work reliably, and a very powerless server.\u003c/li\u003e\n\u003cli\u003eKerberos is a bitch.\u003c/li\u003e\n\u003c/ul\u003e","title":"Deploying Pentaho jobs into Production"},{"content":"Task You have a denormalized table and want to extract a column into a dimension table.\nCaveat You have to keep the ids.\nExtra-Caveat You use an Oracle database.\nExample CARS    id manufacturer_id model_id color     1 1 1 \u0026lsquo;blue\u0026rsquo;   2 1 4 \u0026lsquo;red\u0026rsquo;   3 2 6 \u0026lsquo;black\u0026rsquo;   4 2 8 \u0026lsquo;red\u0026rsquo;    becomes\nCARS    id manufacturer_id model_id color_id     1 1 1 1   2 1 4 2   3 2 6 3   4 2 8 2    COLORS    id name     1 \u0026lsquo;blue\u0026rsquo;   2 \u0026lsquo;red\u0026rsquo;   3 \u0026lsquo;black\u0026rsquo;    Solution:\n-- Create new table from Select-Statement CREATE TABLE colors AS SELECT ROWNUM as id, name FROM (SELECT color FROM cars GROUP BY color); -- Add constraints on newly create table CREATE UNIQUE INDEX colors_id_uindex ON colors(id); ALTER TABLE colors ADD CONSTRAINT colors_id_pk PRIMARY KEY (id); -- Add reference from fact-table to new dimension ALTER TABLE cars ADD color_id NUMBER DEFAULT NULL NULL; UPDATE cars SET color_id = (SELECT id FROM colors WHERE colors.id = cars.color); -- Delete original column ALTER TABLE cars DROP COLUMN color; ","permalink":"https://www.jannikarndt.de/blog/2017/03/extracting_dimensions_from_an_oracle_database_table/","summary":"\u003ch3 id=\"task\"\u003eTask\u003c/h3\u003e\n\u003cp\u003eYou have a denormalized table and want to extract a column into a dimension table.\u003c/p\u003e\n\u003ch3 id=\"caveat\"\u003eCaveat\u003c/h3\u003e\n\u003cp\u003eYou have to keep the ids.\u003c/p\u003e\n\u003ch3 id=\"extra-caveat\"\u003eExtra-Caveat\u003c/h3\u003e\n\u003cp\u003eYou use an Oracle database.\u003c/p\u003e","title":"Extracting Dimensions from an Oracle Database Table"},{"content":"I cannot believe that googleing “talend does not work” does not find anything helpful. With this entry I try to fill that void in the internet.\nFail 1: Download You can download talend on their website — except you can\u0026rsquo;t. After filling out all the information (pro-tip: store them in LastPass to insert them the next couple of times) you have to accept the terms of use—except you can\u0026rsquo;t!\nThere is no event listener. What the\u0026hellip;?\nThere\u0026rsquo;s a trick: Select the \u0026ldquo;Phone\u0026rdquo;-field and alt + tab until you arrive at the checkbox, then hit space.\nNext: Enjoy your modern webbrowser prohibiting a site to cross-reference http from https\nBut there is a download-link hidden inside the plethora of error messages:\nIf you\u0026rsquo;re looking for these, here you go:\nhttps://www.opensourceetl.net/eval/621/TalendToolsStudio-20160704_1411-V6.2.1-installer-mac-dlm.dmg https://www.opensourceetl.net/eval/621/TalendToolsStudio-20160704_1411-V6.2.1-linux-x64-installer.run https://www.opensourceetl.net/eval/621/TalendToolsStudio-20160704_1411-V6.2.1-installer-win-dlm.exe  They won\u0026rsquo;t help you, because they might be offline. If not, you get one of the greatest tools of all times—not.\nFail 2: The Download-Tool Pretty simple: You start it, it does nothing:\nReminds me of “A Sharepoint that does throw errors is a good Sharepoint”.\nFail 3: The ZIP-File Okay, while you wait for the download tool to do anything, you can also try out the ZIP-download-option. This link surprisingly worked! And it contains a Talend-Studio-macosx-cocoa.app file!\nAn unsigned one, of course, because why make things easy? The common advise you find on the internet is to turn off Gatekeeper, Apples system tool to prevent you from running malicious code (i.e. from developers who do not pay apple 100$/year). The easy way to get around this is to right-click a file and then click open. One would suspect this to do the exact same thing as double clicking the file, but watch and learn: It gives you an extra button:\nAnd the next thing you see is: “The Talend-Studio-macosx-cocoa executable launcher was unable to locate its companion share library”.\nWHAT?\nThis is a hard one: You need to right-click the app, choose Show package contents, navigate to Contents:MacOS:Talend-Studio-macosx-cocoa and execute that. Voilà! It starts!\n","permalink":"https://www.jannikarndt.de/blog/2017/01/talend_does_not_work/","summary":"\u003cp\u003eI cannot believe that googleing “talend does not work” does not find \u003cem\u003eanything\u003c/em\u003e helpful. With this entry I try to fill that void in the internet.\u003c/p\u003e","title":"talend does not work"},{"content":"#1 Do not fix your code. Rather understand why nothing kept you from creating this bug. Make your code so easy that this bug would have been obvious the first time.\n#2 Automate early. You know, CI/CD. Or just clean-up-scripts. Or a complete infrastructure-as-code. Remember: A script to setup something is the best documentation!\n#3 A function must not do more than one thing. If a function name contains “and” there\u0026rsquo;s still work to do. Build small pieces.\n#4 Side effects are the root of all evil. If you need side effects (like database or file outputs), let it be the only thing a function does.\n#5 It\u0026rsquo;s all about data. Business logic is just a concept to change data.\n#6 A system is defined by its input and output. Every description of anything should focus on these two things first.\n","permalink":"https://www.jannikarndt.de/blog/2017/01/lessons_learned_in_2016/","summary":"\u003ch2 id=\"1\"\u003e#1\u003c/h2\u003e\n\u003ch3 id=\"do-not-fix-your-code\"\u003eDo not fix your code.\u003c/h3\u003e\n\u003cp\u003eRather understand why nothing kept you from creating this bug. Make your code so easy that this bug would have been obvious the first time.\u003c/p\u003e","title":"Lessons learned in 2016"},{"content":"Being the IT-guy aka personal first-level-support™ for way more people than I am comfortable with, I have held quite a few mobile phones in my hand and stared into the abyss that is their home screen. The home screen is the modern view into someone\u0026rsquo;s soul. In a post-privacy-world it is probably one of the most private things we have, since it is utterly worthless to someone who does not interact with it on an hourly basis and has grown to live with whatever way the apps are scattered around the screen.\nI am also the guy who tends to optimize some things beyond any reasonable limit. Thus I present to you:\nThe KPIs of Home Screen Layout The Key Performance Indicators for the home screen are:\n Number of average page swipes Number of average folder-clicks Individuality of color compared to the up to eight apps around it Reachability with the right thumb Compatibility with other apps on same page   Always remember: We optimize for speed, i.e., the time span from I want to open [App] to actually clicking the right App.\n According to the KPIs, the ideal system has\n one page, no folders, different colors, more important apps at the bottom, very similar apps.  If you look at the home screen Apple ships their iPhones with you start to realize that I am not the only crazy person who ever thought about this:\n Side note: Considering the importance of apps at the bottom, do not think user, think Company that sells apps, music and books.\n  Considering the colors: Messages used to send only SMS, which were closely related to the phone-functions. Before visual voicemail you would get an SMS for a missed call. Also note that WhatsApp uses the same color and shading.\n Using the KPIs No what about this perfect layout? As you might know from the wisdom that James Mickens has brought us, there is just one true metric for how good a developer is: window placement. (If you didn\u0026rsquo;t know, watch and learn!)\nThis can be applied to the mobile world as well. And while the concrete implementation is highly personal and depends on how passionately you download every single free App of the Foo, I have found that for every category of apps there are only about three I use, let\u0026rsquo;s say, every week. So the rule we can deduct from all this is:\n one row per category direct access to the three most important apps adjacent to those a folder with the rest of the category  The last point actually allows to use the folder name as the heading for a row, in case you need to start searching.\nResult And now I present to you: the view into my soul:\nThe most important apps are located in the left-most column, which is, I know, not ideal for my right thumb, but I read from left to right and favor starting with the most important information. Also, all the Apple system apps are missing. They are on the first page, since, as I pointed out, Apple has done a sufficient job at placing them.\nI am actively enjoying this layout for a week now, and so far it has saved me approximately 10 minutes of thinking “what else did I want to look at”. I started using that extra time to optimize the mess that lurks on page three, aka the dump.\n","permalink":"https://www.jannikarndt.de/blog/2016/11/perfect_home_screen_layout/","summary":"\u003cp\u003eBeing \u003cem\u003ethe IT-guy aka personal first-level-support\u003c/em\u003e™ for way more people than I am comfortable with, I have held quite a few mobile phones in my hand and stared into the abyss that is their home screen. The home screen is the modern view into someone\u0026rsquo;s soul. In a post-privacy-world it is probably one of the most private things we have, since it is utterly worthless to someone who does not interact with it on an hourly basis and has grown to live with whatever way the apps are scattered around the screen.\u003c/p\u003e","title":"Perfect Home Screen Layout"},{"content":"(via I Love Programming)\nThank goodness, we don\u0026rsquo;t do production.\n","permalink":"https://www.jannikarndt.de/blog/2016/11/bug_fixing_in_production/","summary":"\u003cimg src=\"/blog/2016/11/bugfixing_in_production.png\" alt=\"\"\u003e \n\u003cp\u003e(via \u003ca href=\"https://www.facebook.com/IFreakingLoveProgramming/photos/a.731375420306757.1073741828.731359570308342/1024753800968916/?type=3\u0026amp;theater\"\u003eI Love Programming\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eThank goodness, we don\u0026rsquo;t do \u003cem\u003eproduction\u003c/em\u003e.\u003c/p\u003e","title":"Bug Fixing in Production"},{"content":"Since I bought my personal domain name around 2003, I went through several web-solutions, using static html pages, php pages, a custom designed php cms, finally Wordpress and now, as of yesterday, I am back to static html. The 2016-flavour however, which is another attempt of separation of presentation and content (a concept I highly endorse as a LaTeX user).\nMy main reason was though, that I was spending more time updating Wordpress, its plugins and themes than acutal content.\nSince quite a lot of research, trial-and-error and configuration have lead to the current model, in this post I will share the details with anyone willing to upgrade her website to a 2016-system (with only one and a half months left).\nFrom plain text to html The basis is a bunch of markdown-files, go-templates and the hugo-templating engine. I have tried and failed hugo before, in October, because there was no easy way to debug anything, and my preferred way of learning is through debugging, as opposed to reading the docs. My inability to take any given template as it is makes it worse.\nThis time however I found a template I was quite pleased with. The starting point is the following directory-structure:\nmaster ├─ content (empty) ├─ public (where the results are generated into) ├─ static | ├─ favicons (for iOS, Android and MS) | ├─ img | | └─ header.jpg | └─ favicon.ico ├─ themes | └─ hugo-creative-theme (git clone git@github.com:digitalcraftsman/hugo-creative-theme.git) | ├─ layouts (where to go-magic happens) | | ├─ partials (layout of individual sections) | | | └─ ... | | └─ index.html (coarse structure) | └─ static (for css, js, etc) └─ config.toml (acutal content) I fought long and hard to press my content into to config.toml, I\u0026rsquo;ll spare you the details and refere to the actual file. What I really like is being able to create a custom structure for each content element, for example a software reference consist of\n[[params.services.list]] icon = \u0026#34;https://jannikarndt.github.io/Canal/bridge.png\u0026#34; title = \u0026#34;Canal\u0026#34; description = \u0026#34;A free and open-source COBOL editor and analysis tool\u0026#34; link = \u0026#34;https://jannikarndt.github.io/Canal/\u0026#34; whereas a publication looks like this:\n[[params.publications.years.entry]] text = \u0026#34;Jannik Arndt: “Musicista — A Framework for Computational Musicology”. Masterarbeit, 2014.\u0026#34; link = \u0026#34;https://www.jannikarndt.de/publikationen/musicista/\u0026#34; All software entries are displayed via the following template\n\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; {{ range .Site.Params.services.list }} \u0026lt;a href=\u0026#34;{{ .link }}\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-lg-3 col-md-6 text-center\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;service-box\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ .icon }}\u0026#34; width=\u0026#34;50%\u0026#34; /\u0026gt; \u0026lt;h3\u0026gt;{{ .title }}\u0026lt;/h3\u0026gt; {{ with .description }} \u0026lt;p class=\u0026#34;text-muted\u0026#34;\u0026gt;{{ . }}\u0026lt;/p\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; while publications use\n\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; {{ range .Site.Params.publications.years }} \u0026lt;div class=\u0026#34;col-lg-10\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;{{ .year }}\u0026lt;/h2\u0026gt; {{ range .entry }} \u0026lt;p class=\u0026#34;text-muted\u0026#34;\u0026gt; {{ .text | markdownify }} {{ if .link }} \u0026lt;a href=\u0026#34;{{ .link }}\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;fa fa-external-link\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/p\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; The hugo-engine itselft supports my trial-and-error-approach wonderfully by offering the hugo serve command, which creates a webserver and updates the generated pages instantly if anything changes.\nDeployment (1) I have gathered a little experience with hugo deployment on github pages while working on NiceToKnowIT, and there is also an extensive guide on the hugo site. The problem is\n github pages requires the content at the root of a repository hugo creates the content into the public folder both are non-negotiable  The workaround is to “embed” the gh-pages repository (or branch) in the public folder of the source repository. Apparently there are two ways to do this, submodule and subtree. I went ahead and chose the wrong way (1, 2, 3, 4, 5, I read none of those, but I now know how to use the --force): submodule. The steps are easy:\n delete public folder `git submodule add -b master git@github.com:JannikArndt/jannikarndt.github.io.git public`` add, commit, push cd into public add, commit, push if it worked: never touch a running system. else: use the --force.  subtree is on my todo list, promise.\nIf everything worked you now have\n a repository with two branches:  source for the complete thing master for the generated website (master for the account-pages, gh-pages for repository pages)   a website at https://jannikarndt.github.io  Deployment (2) The next step is to combine the website with my url. My first approach was RedirectMatch in the .htaccess file:\nRedirectMatch 301 ^/$ https://jannikarndt.github.io The idea: I still want to access the old wordpress and a file static files such as papers. The problem: This matches www.jannikarndt.de/ but not www.jannikarndt.de. Yep. The slash does make a difference.\nWhile experimenting with my .htaccess file I also developed an aversion against paying for a domain and then only redirecting it. But, as the professional programmer™ I am I really don\u0026rsquo;t want redundancy, except if automatically generated from a single source. So I decided to dive into uncharted waters and set up a deployment service. (I know, as opposed to starting an FTP client…)\nGithub offers several integrations, one of which is DeployHQ. They offer 10 builds a day for free, which, for now, should suffice, a very clean and easy-to-use website and a hook, which triggers deployment after every push.\nMarvellous! Now I can leave a new version of my website with a single blog entry to rot until the next big thing (IoT + machine learning = personal website?) comes along.\n","permalink":"https://www.jannikarndt.de/blog/2016/11/the_2016-personal-website_infrastructure/","summary":"\u003cp\u003eSince I bought my personal domain name around 2003, I went through several web-solutions,\nusing static html pages, php pages, a custom designed php cms, finally Wordpress and now, as of yesterday, I am back to static html. The 2016-flavour however, which is another attempt of separation of presentation and content (a concept I highly endorse as a LaTeX user).\u003c/p\u003e","title":"The 2016-Personal-Website Infrastructure"},{"content":"\n         1989 Löptin   1999 – 2008 Preetz   2008 – 2015 Oldenburg\nB.A. Musikwissenschaften, M.Sc. Informatik   2015 – 2016 PPI, Hamburg At PPI I learned to build software to specifications. I became an expert in C#, learned to understand COBOL and through that, discovered the beauty of functional programming in Scala.   2017 – 2019 holisticon, Hamburg\nMy work at holisticon taught me an agile mindset, created excitement for Big Data and airplanes at LHT, let me experience the Data Science and the Data Engineering parts of a huge project at Breuninger, fostered my Scala skills and finally led me to applying all that I learned at MOIA, in the team that cared about every step in the customer journey.   2019 – today MOIA, Hamburg\nI am…\n… a Product-Engineer: I am one of the minds that make booking a MOIA a good experience\n… a Scala-Dev: I nerd about making advanced code readable\n… a Chapter-Lead: I’m a servant leader to a handful amazing devs\nI try to understand all, from the infrastructure and AWS services, over the code and it’s dependencies, the pipelines and processes, the teams and APIs, to the apps and our users. And I enjoy sharing this knowledge, in onboarding sessions for the newbies, architecture discussions with the seniors or dashboards and slides for the pros.\n    I am good at understanding complex systems and chopping them down into smaller solutions. I enjoy making this look easy by explaining it well. I gain energy on working with people who care, on a product that matters.\n","permalink":"https://www.jannikarndt.de/about/","summary":"about","title":"About"}]